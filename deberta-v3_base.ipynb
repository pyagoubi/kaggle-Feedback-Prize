{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"SAVE_PATH = './'\nTRAIN_PATH = '../input/feedback-prize-english-language-learning/train.csv'\nTEST_PATH = '../input/feedback-prize-english-language-learning/test.csv'\nSAMPLE_SUB_PATH = '../input/feedback-prize-english-language-learning/sample_submission.csv' \n\nTARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"id":"sBcRQkP8jGL2"}},{"cell_type":"code","source":"%%capture\n!pip install iterative-stratification\n!pip install sentencepiece\n!pip install transformers==4.21.2\n#!pip install iterative-stratification --no-index --find-links=file:../input/iterstratification/iterstrat\n\nimport warnings\nimport sentencepiece\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000) \nfrom tqdm import tqdm\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom torch import autocast\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, BertModel, BertTokenizer\n\nprint('Transformer Version: ', transformers.__version__)","metadata":{"id":"KxsDW5bmDL3v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    model= 'microsoft/deberta-v3-base'\n    gradient_checkpointing=True\n    epochs=10\n    eps=1e-6\n    num_workers=4\n    batch_size=2\n    weight_decay=0.01\n    target_cols=TARGET_COLS\n    seed=42\n    n_fold=4\n    train=True\n    #scheduler='cosine' # ['linear', 'cosine']\n    #batch_scheduler=True\n    #num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    print_freq = 100\n    #max_len=512\n\n\n\n\n\ndef import_data(tr =TRAIN_PATH, te =TEST_PATH, sample =SAMPLE_SUB_PATH ):\n  df_train = pd.read_csv(tr)\n  df_test = pd.read_csv(te)\n  submission = pd.read_csv(sample)\n  return df_train, df_test, submission\n\ndef replace_nl(df_train, df_test):\n  df_train['full_text'] = df_train['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl= r'', regex=True)\n  df_test['full_text'] = df_test['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl=r'', regex=True)\n  return df_train, df_test\n\ndef set_folds(df_train):\n  Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n  for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[cfg.target_cols])):\n      df_train.loc[val_index, 'fold'] = int(n)\n  df_train['fold'] = df_train['fold'].astype(int)\n  display(df_train.groupby('fold').size())\n  return df_train\n\ndef load_prepare():\n  df_train, df_test, submission = import_data()\n  df_train, df_test = replace_nl(df_train, df_test)\n  df_train=  set_folds(df_train)\n  return df_train, df_test, submission\n\n","metadata":{"id":"JpNOcDU65l2q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test, submission = load_prepare()","metadata":{"id":"oi4-ULuW8-7B","outputId":"556065df-4d40-420c-8cdd-6b804c5282cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg.model)\ntokenizer.save_pretrained(SAVE_PATH+'tokenizer/')\ncfg.tokenizer = tokenizer\n\n\n\n#Dataset Deberta Base\nclass Dataset_Db(torch.utils.data.Dataset):\n\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.labels = df[cfg.target_cols].values\n        self.texts = df[[\"full_text\"]].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        batch_texts = cfg.tokenizer(self.texts[idx][0], \n                                padding='max_length', \n                                max_length = 1450, \n                                truncation=True, \n                                return_tensors=None, \n                                add_special_tokens=True,\n                                pad_to_max_length=True                     \n                                )\n        \n        for k, v in batch_texts.items():\n          batch_texts[k] = torch.tensor(v, dtype=torch.long)\n\n        batch_y = torch.tensor(self.labels[idx], dtype=torch.float)\n        return batch_texts, batch_y\n\n\n#Model Deberta Base\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass DBB(nn.Module):\n  def __init__(self, cfg):\n    super().__init__()\n    self.cfg = cfg\n    self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n    self.config.hidden_dropout = 0.\n    self.config.hidden_dropout_prob = 0.\n    self.config.attention_dropout = 0.\n    self.config.attention_probs_dropout_prob = 0.\n    self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n    self.pool = MeanPooling()\n    self.out = nn.Linear(self.config.hidden_size, 6)\n    self._init_weights(self.out)\n\n  def _init_weights(self, module):\n      if isinstance(module, nn.Linear):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.bias is not None:\n              module.bias.data.zero_()\n      elif isinstance(module, nn.Embedding):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.padding_idx is not None:\n              module.weight.data[module.padding_idx].zero_()\n      elif isinstance(module, nn.LayerNorm):\n          module.bias.data.zero_()\n          module.weight.data.fill_(1.0)\n\n  def forward(self, inputs):\n      outputs = self.model(**inputs)\n      last_hidden_states = outputs[0]\n      pooled_output = self.pool(last_hidden_states, inputs['attention_mask'])\n      final_out = self.out(pooled_output)\n      return final_out\n    \n# ====================================================\n#####Loss\n#====================================================\nclass RMSELoss(nn.Module):\n    def __init__(self, reduction='mean', eps=1e-9):\n        super().__init__()\n        self.mse = nn.MSELoss(reduction='none')\n        self.reduction = reduction\n        self.eps = eps\n\n    def forward(self, y_pred, y_true):\n        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n        if self.reduction == 'none':\n            loss = loss\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n        return loss\n\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n          'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n","metadata":{"id":"vqhigdnNAGYq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"LU84H01xiSZc"}},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler()\n\n\nfor val_fold in range(cfg.n_fold):\n    oof_df = pd.DataFrame()\n\n\n    train_folds = df_train[df_train['fold'] != val_fold].reset_index(drop=True)\n    valid_folds = df_train[df_train['fold'] == val_fold].reset_index(drop=True)\n    valid_labels = valid_folds[cfg.target_cols].values\n\n    train_dataset = Dataset_Db(cfg, train_folds)\n    valid_dataset = Dataset_Db(cfg, valid_folds)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=cfg.batch_size,\n                              shuffle=True,\n                              num_workers=cfg.num_workers, \n                              pin_memory=True#, \n                              #drop_last=True\n                              )\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=cfg.batch_size * 2,\n                              shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n\n    model = DBB(cfg)\n    #torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=cfg.encoder_lr, \n                                                decoder_lr=cfg.decoder_lr,\n                                                weight_decay=cfg.weight_decay)\n\n    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n\n    criterion = RMSELoss() #RMSELoss(reduction=\"mean\")\n\n    best_score = np.inf\n    \n\n    for epoch in range(cfg.epochs):\n        \n        model.train()\n        scaler = torch.cuda.amp.GradScaler(enabled=True)\n\n        losses = []\n        counter = 0\n\n        #train\n        for step, (inputs, labels) in enumerate(train_loader):\n            inputs = collate(inputs)\n            for k, v in inputs.items():\n                inputs[k] = v.to(device)\n\n            labels = labels.to(device)\n            batch_size = labels.size(0)\n            \n            #with torch.cuda.amp.autocast(enabled=True):\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n\n            losses.append(loss*batch_size)\n            counter += batch_size\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            total = sum(losses)/counter\n\n            if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n                print(f'Epoch: [{epoch}][{step}/{len(train_loader)}]  \\n',\n                    f'Loss: {total}')\n\n        \n\n      #validation\n\n        val_losses = []\n        val_counter = 0\n        preds = []\n        model.eval()\n\n        for step, (inputs, labels) in enumerate(valid_loader):\n            inputs = collate(inputs)\n\n            for k, v in inputs.items():\n                inputs[k] = v.to(device)\n\n            labels = labels.to(device)\n            batch_size = labels.size(0)\n\n            with torch.no_grad():\n                val_y_preds = model(inputs)\n                val_loss = criterion(val_y_preds, labels)\n\n            val_losses.append(val_loss*batch_size)\n            val_counter += batch_size\n\n            total_val = sum(val_losses)/val_counter\n            preds.append(val_y_preds.to('cpu').numpy())\n\n\n\n    predictions = np.concatenate(preds)\n    total_val_loss = sum(val_losses)/val_counter\n    print(f'***************EVAL: Loss: {total_val_loss}')\n\n\n    if best_score > total_val_loss:\n        best_score = total_val_loss\n        torch.save({'model': model.state_dict(),\n                      'predictions': predictions},\n                      SAVE_PATH+f\"{cfg.model.replace('/', '-')}_fold{val_fold}_best.pth\")\n\n    del model\n\n\n\n\n\n\n\n\n\n\n\n\n\n        ","metadata":{"id":"3IGJPW0jF1m4","outputId":"fa2a0771-7593-428e-dfcc-8fdedfdc2a47","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"nVgpoeUSPfCh"},"execution_count":null,"outputs":[]}]}