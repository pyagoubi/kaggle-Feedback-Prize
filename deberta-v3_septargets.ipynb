{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/pyagoubi/kaggle-Feedback-Prize/blob/main/Debertav3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"#Mounting Google Drive\n#from google.colab import drive\n#drive.mount('/content/drive')\n\n#import os\n#os.chdir('/content/drive/MyDrive/kaggle Feedback')","metadata":{"id":"VuH7idBliYP8","outputId":"fa563cce-d351-415f-eb19-e01b01264dce","execution":{"iopub.status.busy":"2022-10-10T05:48:45.158784Z","iopub.execute_input":"2022-10-10T05:48:45.159247Z","iopub.status.idle":"2022-10-10T05:48:45.180674Z","shell.execute_reply.started":"2022-10-10T05:48:45.159151Z","shell.execute_reply":"2022-10-10T05:48:45.179741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVE_PATH = './'\nTRAIN_PATH = '../input/feedback-prize-english-language-learning/train.csv'\nTEST_PATH = '../input/feedback-prize-english-language-learning/test.csv'\nSAMPLE_SUB_PATH = '../input/feedback-prize-english-language-learning/sample_submission.csv' \n\nTARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"id":"sBcRQkP8jGL2","execution":{"iopub.status.busy":"2022-10-10T05:48:45.183009Z","iopub.execute_input":"2022-10-10T05:48:45.183680Z","iopub.status.idle":"2022-10-10T05:48:45.192663Z","shell.execute_reply.started":"2022-10-10T05:48:45.183644Z","shell.execute_reply":"2022-10-10T05:48:45.191655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install iterative-stratification\n!pip install sentencepiece\n!pip install transformers==4.21.2\n#!pip install iterative-stratification --no-index --find-links=file:../input/iterstratification/iterstrat\n\nimport warnings\nimport sentencepiece\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000) \nfrom tqdm import tqdm\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom torch import autocast\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, BertModel, BertTokenizer\n\nprint('Transformer Version: ', transformers.__version__)","metadata":{"id":"KxsDW5bmDL3v","execution":{"iopub.status.busy":"2022-10-10T05:48:45.194227Z","iopub.execute_input":"2022-10-10T05:48:45.194575Z","iopub.status.idle":"2022-10-10T05:49:24.869911Z","shell.execute_reply.started":"2022-10-10T05:48:45.194541Z","shell.execute_reply":"2022-10-10T05:49:24.868749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    model= 'microsoft/deberta-v3-base'\n    gradient_checkpointing=True\n    epochs=10\n    eps=1e-6\n    num_workers=4\n    batch_size=10\n    weight_decay=0.01\n    target_cols=TARGET_COLS\n    seed=42\n    n_fold=4\n    train=True\n    #scheduler='cosine' # ['linear', 'cosine']\n    #batch_scheduler=True\n    #num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    print_freq = 100\n    #max_len=512\n\n\n\n\n\ndef import_data(tr =TRAIN_PATH, te =TEST_PATH, sample =SAMPLE_SUB_PATH ):\n  df_train = pd.read_csv(tr)\n  df_test = pd.read_csv(te)\n  submission = pd.read_csv(sample)\n  return df_train, df_test, submission\n\ndef replace_nl(df_train, df_test):\n  df_train['full_text'] = df_train['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl= r'', regex=True)\n  df_test['full_text'] = df_test['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl=r'', regex=True)\n  return df_train, df_test\n\ndef set_folds(df_train):\n  Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n  for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[cfg.target_cols])):\n      df_train.loc[val_index, 'fold'] = int(n)\n  df_train['fold'] = df_train['fold'].astype(int)\n  display(df_train.groupby('fold').size())\n  return df_train\n\ndef load_prepare():\n  df_train, df_test, submission = import_data()\n  df_train, df_test = replace_nl(df_train, df_test)\n  df_train=  set_folds(df_train)\n  return df_train, df_test, submission\n\n","metadata":{"id":"JpNOcDU65l2q","execution":{"iopub.status.busy":"2022-10-10T05:49:24.872629Z","iopub.execute_input":"2022-10-10T05:49:24.873762Z","iopub.status.idle":"2022-10-10T05:49:24.889457Z","shell.execute_reply.started":"2022-10-10T05:49:24.873711Z","shell.execute_reply":"2022-10-10T05:49:24.888278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test, submission = load_prepare()","metadata":{"id":"oi4-ULuW8-7B","outputId":"556065df-4d40-420c-8cdd-6b804c5282cc","execution":{"iopub.status.busy":"2022-10-10T05:49:24.891264Z","iopub.execute_input":"2022-10-10T05:49:24.891956Z","iopub.status.idle":"2022-10-10T05:49:25.350442Z","shell.execute_reply.started":"2022-10-10T05:49:24.891910Z","shell.execute_reply":"2022-10-10T05:49:25.349523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset Deberta Base\nclass Dataset_Db(torch.utils.data.Dataset):\n\n    def __init__(self, df, target, train = True):\n        self.train = train\n        self.target = target\n        if self.train: self.labels = df[self.target].values\n        self.texts = df[[\"full_text\"]].values\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        batch_texts = self.tokenizer(self.texts[idx][0], \n                                padding='max_length', \n                                max_length = 512, truncation=True#, \n                                #return_tensors=\"pt\"\n                                )\n        \n        for k, v in batch_texts.items():\n          batch_texts[k] = torch.tensor(v, dtype=torch.long)\n\n        if self.train: batch_y = torch.tensor(self.labels[idx], dtype=torch.float)\n        if self.train: return batch_texts, batch_y\n        else: return batch_texts\n\n#Model Deberta Base\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass DBB(nn.Module):\n  def __init__(self, cfg):\n    super().__init__()\n    self.cfg = cfg\n    self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n    self.model = AutoModel.from_pretrained(cfg.model)\n    self.pool = MeanPooling()\n    # self.linear = nn.Linear(self.config.hidden_size, 512)\n    # self.dropout = nn.Dropout(p=0.1)\n    # self.relu = nn.ReLU()\n    self.out = nn.Linear(self.config.hidden_size, 1)\n    self._init_weights(self.out)\n\n  def _init_weights(self, module):\n      if isinstance(module, nn.Linear):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.bias is not None:\n              module.bias.data.zero_()\n      elif isinstance(module, nn.Embedding):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.padding_idx is not None:\n              module.weight.data[module.padding_idx].zero_()\n      elif isinstance(module, nn.LayerNorm):\n          module.bias.data.zero_()\n          module.weight.data.fill_(1.0)\n\n  def forward(self, inputs):\n      outputs = self.model(**inputs)\n      last_hidden_states = outputs[0]\n      pooled_output = self.pool(last_hidden_states, inputs['attention_mask'])\n      final_out = self.out(pooled_output)\n      return final_out\n    \n# ====================================================\n# Loss\n# ====================================================\n# class RMSELoss(nn.Module):\n#     def __init__(self, reduction='mean', eps=1e-9):\n#         super().__init__()\n#         self.mse = nn.MSELoss(reduction='none')\n#         self.reduction = reduction\n#         self.eps = eps\n\n#     def forward(self, y_pred, y_true):\n#         loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n#         if self.reduction == 'none':\n#             loss = loss\n#         elif self.reduction == 'sum':\n#             loss = loss.sum()\n#         elif self.reduction == 'mean':\n#             loss = loss.mean()\n#         return loss\n\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n          'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\n","metadata":{"id":"vqhigdnNAGYq","execution":{"iopub.status.busy":"2022-10-10T05:49:25.352267Z","iopub.execute_input":"2022-10-10T05:49:25.353077Z","iopub.status.idle":"2022-10-10T05:49:25.374030Z","shell.execute_reply.started":"2022-10-10T05:49:25.353037Z","shell.execute_reply":"2022-10-10T05:49:25.372888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"LU84H01xiSZc"}},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler()\n\nfor target in cfg.target_cols:\n  for val_fold in range(cfg.n_fold):\n    oof_df = pd.DataFrame()\n\n\n    train_folds = df_train[df_train['fold'] != val_fold].reset_index(drop=True)\n    valid_folds = df_train[df_train['fold'] == val_fold].reset_index(drop=True)\n    valid_labels = valid_folds[target].values\n      \n    train_dataset = Dataset_Db(train_folds, target)\n    valid_dataset = Dataset_Db(valid_folds, target)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=cfg.batch_size,\n                              shuffle=True,\n                              num_workers=cfg.num_workers, \n                              pin_memory=True#, \n                              #drop_last=True\n                              )\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=cfg.batch_size * 2,\n                              shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n    \n    model = DBB(cfg)\n    #torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=cfg.encoder_lr, \n                                                decoder_lr=cfg.decoder_lr,\n                                                weight_decay=cfg.weight_decay)\n    \n    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n\n    criterion = nn.MSELoss() #RMSELoss(reduction=\"mean\")\n\n    best_score = np.inf\n\n    for epoch in range(cfg.epochs):\n      \n      model.train()\n      scaler = torch.cuda.amp.GradScaler(enabled=True)\n\n      losses = []\n      counter = 0\n\n      #train\n      for step, (inputs, labels) in enumerate(train_loader):\n        \n\n\n          for k, v in inputs.items():\n              inputs[k] = v.to(device)\n\n          labels = labels.to(device)\n          batch_size = labels.size(0)\n\n          with torch.cuda.amp.autocast(enabled=True):\n              y_preds = model(inputs)\n              loss = criterion(y_preds, labels)\n\n          losses.append(loss*batch_size)\n          counter += batch_size\n\n          scaler.scale(loss).backward()\n          scaler.step(optimizer)\n          scaler.update()\n          optimizer.zero_grad()\n          total = sum(losses)/counter\n\n          if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n              print(f'Epoch: [{epoch}][{step}/{len(train_loader)}]  \\n',\n                    f'Loss: {total}')\n          \n          \n      #validation\n\n      val_losses = []\n      val_counter = 0\n      preds = []\n      model.eval()\n\n      for step, (inputs, labels) in enumerate(valid_loader):\n\n        for k, v in inputs.items():\n              inputs[k] = v.to(device)\n\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n\n        with torch.no_grad():\n            val_y_preds = model(inputs)\n            val_loss = criterion(val_y_preds, labels)\n\n        val_losses.append(val_loss*batch_size)\n        val_counter += batch_size\n\n        total_val = sum(val_losses)/val_counter\n        preds.append(val_y_preds.to('cpu').numpy())\n\n            \n\n    predictions = np.concatenate(preds)\n    total_val_loss = sum(val_losses)/val_counter\n    print(f'EVAL: Loss: {total_val_loss}')\n\n\n    if best_score > total_val_loss:\n      best_score = total_val_loss\n      torch.save({'model': model.state_dict(),\n                  'predictions': predictions},\n                  SAVE_PATH+f\"{cfg.model.replace('/', '-')}_t{target}_fold{val_fold}_best.pth\")\n      \n    del model\n  \n\n\n\n\n\n\n\n\n\n\n\n\n        ","metadata":{"id":"3IGJPW0jF1m4","outputId":"fa2a0771-7593-428e-dfcc-8fdedfdc2a47","execution":{"iopub.status.busy":"2022-10-10T05:49:25.377343Z","iopub.execute_input":"2022-10-10T05:49:25.377927Z","iopub.status.idle":"2022-10-10T13:15:00.297196Z","shell.execute_reply.started":"2022-10-10T05:49:25.377896Z","shell.execute_reply":"2022-10-10T13:15:00.295754Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-10-10T13:29:09.801604Z","iopub.execute_input":"2022-10-10T13:29:09.802582Z","iopub.status.idle":"2022-10-10T14:05:06.550142Z","shell.execute_reply.started":"2022-10-10T13:29:09.802543Z","shell.execute_reply":"2022-10-10T14:05:06.549136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"nVgpoeUSPfCh"},"execution_count":null,"outputs":[]}]}