{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Training Notebook of a Deberte-v3-base model with variable number of mean pooling layers and layerwise lr decay finetuning on FB3 targets.","metadata":{}},{"cell_type":"code","source":"SAVE_PATH = './'\nTRAIN_PATH = '../input/feedback-prize-english-language-learning/train.csv'\nTEST_PATH = '../input/feedback-prize-english-language-learning/test.csv'\nSAMPLE_SUB_PATH = '../input/feedback-prize-english-language-learning/sample_submission.csv' \n\nTARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"execution":{"iopub.status.busy":"2022-11-01T19:08:20.117996Z","iopub.execute_input":"2022-11-01T19:08:20.118610Z","iopub.status.idle":"2022-11-01T19:08:20.145303Z","shell.execute_reply.started":"2022-11-01T19:08:20.118524Z","shell.execute_reply":"2022-11-01T19:08:20.144465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install iterative-stratification\n!pip install sentencepiece\n!pip install transformers==4.21.2\n#!pip install iterative-stratification --no-index --find-links=file:../input/iterstratification/iterstrat\n\nimport warnings\nimport sentencepiece\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000) \nfrom tqdm import tqdm\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom torch import autocast\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, BertModel, BertTokenizer\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint('Transformer Version: ', transformers.__version__)","metadata":{"id":"KxsDW5bmDL3v","execution":{"iopub.status.busy":"2022-11-01T19:08:20.147386Z","iopub.execute_input":"2022-11-01T19:08:20.147839Z","iopub.status.idle":"2022-11-01T19:09:02.083399Z","shell.execute_reply.started":"2022-11-01T19:08:20.147801Z","shell.execute_reply":"2022-11-01T19:09:02.082161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\nclass cfg:\n    model= 'microsoft/deberta-v3-base'\n    gradient_checkpointing=True\n    epochs=15\n    eps=1e-6\n    num_workers=4\n    batch_size=3\n    weight_decay=0.9\n    target_cols=TARGET_COLS\n    seed=42\n    n_fold=4\n    train=True\n    mp_depth = 4 #number of mean poolings\n    num_warmup_steps=0\n    lr=1e-5\n    layer_decay = 0.9\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    print_freq = 1000\n    accumulation_steps = 10\n    max_norm =1000\n\ndef import_data(tr =TRAIN_PATH, te =TEST_PATH, sample =SAMPLE_SUB_PATH ):\n  df_train = pd.read_csv(tr)\n  df_test = pd.read_csv(te)\n  submission = pd.read_csv(sample)\n  return df_train, df_test, submission\n\ndef replace_nl(df_train, df_test):\n  df_train['full_text'] = df_train['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl= r'', regex=True)\n  df_test['full_text'] = df_test['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl=r'', regex=True)\n  return df_train, df_test\n\ndef set_folds(df_train):\n  Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n  for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[cfg.target_cols])):\n      df_train.loc[val_index, 'fold'] = int(n)\n  df_train['fold'] = df_train['fold'].astype(int)\n  display(df_train.groupby('fold').size())\n  return df_train\n\ndef load_prepare():\n  df_train, df_test, submission = import_data()\n  df_train, df_test = replace_nl(df_train, df_test)\n  df_train=  set_folds(df_train)\n  return df_train, df_test, submission","metadata":{"id":"JpNOcDU65l2q","execution":{"iopub.status.busy":"2022-11-01T19:09:02.088802Z","iopub.execute_input":"2022-11-01T19:09:02.092277Z","iopub.status.idle":"2022-11-01T19:09:02.116064Z","shell.execute_reply.started":"2022-11-01T19:09:02.092229Z","shell.execute_reply":"2022-11-01T19:09:02.115078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset \nclass Dataset_Db(torch.utils.data.Dataset):\n\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.labels = df[cfg.target_cols].values\n        self.texts = df[[\"full_text\"]].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        batch_texts = cfg.tokenizer(self.texts[idx][0], \n                                padding='max_length', \n                                max_length = 1450, \n                                truncation=True, \n                                return_tensors=None, \n                                add_special_tokens=True,\n                                pad_to_max_length=True                     \n                                )\n        \n        for k, v in batch_texts.items():\n          batch_texts[k] = torch.tensor(v, dtype=torch.long)\n\n        batch_y = torch.tensor(self.labels[idx], dtype=torch.float)\n        return batch_texts, batch_y\n\n\n#Model\nclass MeanPooling(nn.Module):\n    def __init__(self, mpd):\n        super(MeanPooling, self).__init__()\n        self.mp_depth = mpd\n        \n    def forward(self, last_hidden_state,hidden_states, attention_mask):\n        mp_embeddings = []\n\n        for i in range(self.mp_depth):\n            if i ==0:\n                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1) \n            else:\n                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states[i-1].size()).float()\n                sum_embeddings = torch.sum(hidden_states[i-1] * input_mask_expanded, 1)   \n            sum_mask = input_mask_expanded.sum(1)\n            sum_mask = torch.clamp(sum_mask, min=1e-9)\n            mean_embeddings = sum_embeddings / sum_mask\n            \n            mp_embeddings.append(mean_embeddings)\n    \n        results = torch.cat(mp_embeddings , dim=1)\n        results = results.reshape(results.size(0), self.mp_depth, int(results.size(1)/self.mp_depth))\n        return results\n\n\n\n    \n    \nclass DBB(nn.Module):\n    def __init__(self, cfg, mp_depth):\n        super().__init__()\n        self.cfg = cfg\n        self.mp_depth = mp_depth\n        self.config = AutoConfig.from_pretrained(cfg.model)\n        self.config.output_hidden_states=True\n        self.config.hidden_dropout_prob = 0.\n        self.config.attention_probs_dropout_prob = 0.\n        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        self.pool = MeanPooling(self.mp_depth)\n        self.mpd = nn.Linear(mp_depth, 1)\n        self.out = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.mpd)\n        self._init_weights(self.out)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)        \n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n#         last_hidden_state = outputs[0]\n#         hidden_states = outputs[1]\n        pooled_outputs = self.pool(outputs.last_hidden_state,outputs.hidden_states,  inputs['attention_mask'])\n        pooled_outputs = pooled_outputs.permute(0,2,1)\n        mean_pooled = self.mpd(pooled_outputs)\n        mean_pooled =mean_pooled.squeeze(-1)\n        final_out = self.out(mean_pooled)\n        return final_out\n\n# ====================================================\n#####Loss\n#====================================================\nclass RMSELoss(nn.Module):\n    def __init__(self, reduction='mean', eps=1e-9):\n        super().__init__()\n        self.mse = nn.MSELoss(reduction='none')\n        self.reduction = reduction\n        self.eps = eps\n\n    def forward(self, y_pred, y_true):\n        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n        if self.reduction == 'none':\n            loss = loss\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n        return loss\n\n\n    \ndef get_lr_groups(model, learning_rate=cfg.lr, layer_decay=cfg.layer_decay):\n   \n    n_layers = len(model.model.encoder.layer) + 6 # + 1 (embedding) +2 layernorm.. +2 lin\n\n    embedding_decayed_lr = learning_rate * (layer_decay ** (n_layers+6))\n    grouped_parameters = [{\"params\": model.model.embeddings.parameters(), 'lr': embedding_decayed_lr}]\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    for depth in range(1, n_layers-5):\n        decayed_lr = learning_rate * (layer_decay ** (n_layers + 6 - depth))\n        grouped_parameters.append(\n            {\"params\": [p for n, p in model.model.encoder.layer[depth-1].named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': decayed_lr, 'weight_decay': cfg.weight_decay}\n        )\n        grouped_parameters.append(\n            {\"params\": [p for n, p in model.model.encoder.layer[depth-1].named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': decayed_lr, 'weight_decay': 0.0})\n        \n    #rel embeddings layer\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.model.encoder.rel_embeddings.named_parameters() if not any(nd in n for nd in no_decay)], \n             'lr': learning_rate * (layer_decay ** 4), 'weight_decay': cfg.weight_decay})\n    grouped_parameters.append(\n        {\"params\": [p for n, p in model.model.encoder.rel_embeddings.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': learning_rate * (layer_decay ** 4), 'weight_decay': 0.0})\n    \n    #layer norm layer\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.model.encoder.LayerNorm.named_parameters() if not any(nd in n for nd in no_decay)], \n             'lr': learning_rate * (layer_decay ** 3), 'weight_decay': cfg.weight_decay})\n    grouped_parameters.append(\n        {\"params\": [p for n, p in model.model.encoder.LayerNorm.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': learning_rate * (layer_decay ** 3), 'weight_decay': 0.0})    \n    \n    #Pooling layer\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.pool.named_parameters() if not any(nd in n for nd in no_decay)], \n             'lr': learning_rate * (layer_decay ** 2), 'weight_decay': cfg.weight_decay})\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.pool.named_parameters() if any(nd in n for nd in no_decay)], \n             'lr': learning_rate * (layer_decay ** 2), 'weight_decay': 0.0})  \n    \n\n    #mpd layer\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.mpd.named_parameters() if not any(nd in n for nd in no_decay)], \n             'lr': learning_rate * (layer_decay ** 1), 'weight_decay': cfg.weight_decay})\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.mpd.named_parameters() if any(nd in n for nd in no_decay)], \n             'lr': learning_rate * (layer_decay ** 1), 'weight_decay': 0.0})    \n\n    #out layer\n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.out.named_parameters() if not any(nd in n for nd in no_decay)], \n             'lr': learning_rate, 'weight_decay': cfg.weight_decay }\n            )     \n    grouped_parameters.append(\n            {\"params\": [p for n, p in model.out.named_parameters() if any(nd in n for nd in no_decay)], \n             'lr': learning_rate, 'weight_decay': 0.0 }\n            )    \n       \n    return grouped_parameters\n\n\n    \ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n          'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n","metadata":{"id":"vqhigdnNAGYq","execution":{"iopub.status.busy":"2022-11-01T19:09:02.122829Z","iopub.execute_input":"2022-11-01T19:09:02.123643Z","iopub.status.idle":"2022-11-01T19:09:02.188951Z","shell.execute_reply.started":"2022-11-01T19:09:02.123601Z","shell.execute_reply":"2022-11-01T19:09:02.187896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    scaler = torch.cuda.amp.GradScaler()\n\n\n    for val_fold in range(cfg.n_fold):\n        oof_df = pd.DataFrame()\n\n\n        train_folds = df_train[df_train['fold'] != val_fold].reset_index(drop=True)\n        valid_folds = df_train[df_train['fold'] == val_fold].reset_index(drop=True)\n        valid_labels = valid_folds[cfg.target_cols].values\n\n        train_dataset = Dataset_Db(cfg, train_folds)\n        valid_dataset = Dataset_Db(cfg, valid_folds)\n\n        train_loader = DataLoader(train_dataset,\n                                  batch_size=cfg.batch_size,\n                                  shuffle=True,\n                                  num_workers=cfg.num_workers, \n                                  pin_memory=True#, \n                                  #drop_last=True\n                                  )\n        valid_loader = DataLoader(valid_dataset,\n                                  batch_size=cfg.batch_size * 2,\n                                  shuffle=False,\n                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n\n        model = DBB(cfg,cfg.mp_depth)\n        #torch.save(model.config, OUTPUT_DIR+'config.pth')\n        model.to(device)\n\n        lr_groups = get_lr_groups(model, learning_rate=1e-5)\n\n        optimizer = AdamW(lr_groups, lr=cfg.lr, eps=cfg.eps, betas=cfg.betas)\n\n        criterion = RMSELoss() #RMSELoss(reduction=\"mean\")\n\n        best_score = np.inf\n\n\n        for epoch in range(cfg.epochs):\n\n            model.train()\n            scaler = torch.cuda.amp.GradScaler(enabled=True)\n\n            losses = []\n            counter = 0\n\n            #train\n            for step, (inputs, labels) in enumerate(train_loader):\n                inputs = collate(inputs)\n                for k, v in inputs.items():\n                    inputs[k] = v.to(device)\n\n                labels = labels.to(device)\n                batch_size = labels.size(0)\n\n                #with torch.cuda.amp.autocast(enabled=True):\n                y_preds = model(inputs)\n                loss = criterion(y_preds, labels)\n\n                if cfg.accumulation_steps > 1:\n                    loss = loss / cfg.accumulation_steps\n\n                losses.append(loss*batch_size)\n                counter += batch_size\n\n                scaler.scale(loss).backward()\n                \n#                 # before gradient clipping the optimizer parameters must be unscaled.\n#                 scaler.unscale_(optimizer)\n    \n#                 # perform optimization step\n#                 torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_norm)\n\n                if (step + 1) % cfg.accumulation_steps == 0:\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n\n                total = sum(losses)/counter\n\n                if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n                    print(f'Epoch: [{epoch}][{step}/{len(train_loader)}]  \\n',\n                        f'Loss: {total}')\n\n\n\n          #validation\n\n            val_losses = []\n            val_counter = 0\n            preds = []\n            model.eval()\n\n            for step, (inputs, labels) in enumerate(valid_loader):\n                inputs = collate(inputs)\n\n                for k, v in inputs.items():\n                    inputs[k] = v.to(device)\n\n                labels = labels.to(device)\n                batch_size = labels.size(0)\n\n                with torch.no_grad():\n                    val_y_preds = model(inputs)\n                    val_loss = criterion(val_y_preds, labels)\n                    \n                if cfg.accumulation_steps > 1:\n                    val_loss = val_loss / cfg.accumulation_steps\n\n                val_losses.append(val_loss*batch_size)\n                val_counter += batch_size\n\n                total_val = sum(val_losses)/val_counter\n                preds.append(val_y_preds.to('cpu').numpy())\n\n\n\n            predictions = np.concatenate(preds)\n            total_val_loss = sum(val_losses)/val_counter\n            print(f'***************EVAL: Loss: {total_val_loss}')\n\n\n            if best_score > total_val_loss:\n                best_score = total_val_loss\n                torch.save({'model': model.state_dict(),\n                              'predictions': predictions},\n                              SAVE_PATH+f\"{cfg.model.replace('/', '-')}_fold{val_fold}_m1.pth\")\n\n        del model\n","metadata":{"id":"3IGJPW0jF1m4","outputId":"fa2a0771-7593-428e-dfcc-8fdedfdc2a47","execution":{"iopub.status.busy":"2022-11-01T19:09:02.190636Z","iopub.execute_input":"2022-11-01T19:09:02.191345Z","iopub.status.idle":"2022-11-01T19:09:02.215665Z","shell.execute_reply.started":"2022-11-01T19:09:02.191290Z","shell.execute_reply":"2022-11-01T19:09:02.214587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test, submission = load_prepare()\ntokenizer = AutoTokenizer.from_pretrained(cfg.model)\n#tokenizer.save_pretrained(SAVE_PATH+'tokenizer/')\ncfg.tokenizer = tokenizer\ntrain()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T19:09:02.217587Z","iopub.execute_input":"2022-11-01T19:09:02.218497Z","iopub.status.idle":"2022-11-01T19:09:50.521768Z","shell.execute_reply.started":"2022-11-01T19:09:02.218399Z","shell.execute_reply":"2022-11-01T19:09:50.519729Z"},"trusted":true},"execution_count":null,"outputs":[]}]}