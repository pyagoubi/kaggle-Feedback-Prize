{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyagoubi/kaggle-Feedback-Prize/blob/main/Db_base_2_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Notebook of a Deberte-v3-base model with variable number of mean pooling layers and layerwise lr decay finetuning on FB3 targets."
      ],
      "metadata": {
        "id": "YBcIqbLcQfss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE_PATH = './'\n",
        "# TRAIN_PATH = '../input/feedback-prize-english-language-learning/train.csv'\n",
        "# TEST_PATH = '../input/feedback-prize-english-language-learning/test.csv'\n",
        "# SAMPLE_SUB_PATH = '../input/feedback-prize-english-language-learning/sample_submission.csv' \n",
        "# MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "\n",
        "# TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-01T19:08:20.117996Z",
          "iopub.execute_input": "2022-11-01T19:08:20.118610Z",
          "iopub.status.idle": "2022-11-01T19:08:20.145303Z",
          "shell.execute_reply.started": "2022-11-01T19:08:20.118524Z",
          "shell.execute_reply": "2022-11-01T19:08:20.144465Z"
        },
        "trusted": true,
        "id": "mYMmTq__Qfsv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/kaggle Feedback/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tTI8r28Q0pm",
        "outputId": "ab9ddbcc-3cd9-4a37-f17f-c8c9e0f76e65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = './'\n",
        "TRAIN_PATH = 'train.csv'\n",
        "TEST_PATH = 'test.csv'\n",
        "SAMPLE_SUB_PATH = 'sample_submission.csv' \n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "\n",
        "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']"
      ],
      "metadata": {
        "id": "PgzAxmN_QwJi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install iterative-stratification\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==4.21.2\n",
        "#!pip install iterative-stratification --no-index --find-links=file:../input/iterstratification/iterstrat\n",
        "\n",
        "import warnings\n",
        "import sentencepiece\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000) \n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import autocast\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, BertModel, BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Transformer Version: ', transformers.__version__)"
      ],
      "metadata": {
        "id": "KxsDW5bmDL3v",
        "execution": {
          "iopub.status.busy": "2022-11-01T19:08:20.147386Z",
          "iopub.execute_input": "2022-11-01T19:08:20.147839Z",
          "iopub.status.idle": "2022-11-01T19:09:02.083399Z",
          "shell.execute_reply.started": "2022-11-01T19:08:20.147801Z",
          "shell.execute_reply": "2022-11-01T19:09:02.082161Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "class cfg:\n",
        "    model= MODEL_NAME\n",
        "    gradient_checkpointing=True\n",
        "    epochs=30\n",
        "    eps=1e-6\n",
        "    num_workers=4\n",
        "    batch_size=3\n",
        "    weight_decay=0.9\n",
        "    target_cols=TARGET_COLS\n",
        "    seed=42\n",
        "    n_fold=4\n",
        "    train=True\n",
        "    mp_depth = 4 #number of mean poolings\n",
        "    num_warmup_steps=0\n",
        "    lr=2e-4\n",
        "    layer_decay = 0.9\n",
        "    min_lr=1e-6\n",
        "    eps=1e-6\n",
        "    betas=(0.9, 0.999)\n",
        "    print_freq = 100\n",
        "    accumulation_steps = 50\n",
        "    max_norm =1000\n",
        "    batch_scheduler=True\n",
        "    num_cycles=0.5\n",
        "\n",
        "\n",
        "def import_data(tr =TRAIN_PATH, te =TEST_PATH, sample =SAMPLE_SUB_PATH ):\n",
        "  df_train = pd.read_csv(tr)\n",
        "  df_test = pd.read_csv(te)\n",
        "  submission = pd.read_csv(sample)\n",
        "  return df_train, df_test, submission\n",
        "\n",
        "def replace_nl(df_train, df_test):\n",
        "  df_train['full_text'] = df_train['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl= r'', regex=True)\n",
        "  df_test['full_text'] = df_test['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl=r'', regex=True)\n",
        "  return df_train, df_test\n",
        "\n",
        "def set_folds(df_train):\n",
        "  Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
        "  for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[cfg.target_cols])):\n",
        "      df_train.loc[val_index, 'fold'] = int(n)\n",
        "  df_train['fold'] = df_train['fold'].astype(int)\n",
        "  display(df_train.groupby('fold').size())\n",
        "  return df_train\n",
        "\n",
        "def load_prepare():\n",
        "  df_train, df_test, submission = import_data()\n",
        "  df_train, df_test = replace_nl(df_train, df_test)\n",
        "  df_train=  set_folds(df_train)\n",
        "  return df_train, df_test, submission"
      ],
      "metadata": {
        "id": "JpNOcDU65l2q",
        "execution": {
          "iopub.status.busy": "2022-11-01T19:09:02.088802Z",
          "iopub.execute_input": "2022-11-01T19:09:02.092277Z",
          "iopub.status.idle": "2022-11-01T19:09:02.116064Z",
          "shell.execute_reply.started": "2022-11-01T19:09:02.092229Z",
          "shell.execute_reply": "2022-11-01T19:09:02.115078Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "\n",
        "\n",
        "def class2dict(f):\n",
        "  return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zYd2cAbyIYkf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset \n",
        "class Dataset_Db(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, cfg, df):\n",
        "        self.cfg = cfg\n",
        "        self.labels = df[cfg.target_cols].values\n",
        "        self.texts = df[[\"full_text\"]].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = cfg.tokenizer(self.texts[idx][0], \n",
        "                                padding='max_length', \n",
        "                                max_length = 1450, \n",
        "                                truncation=True, \n",
        "                                return_tensors=None, \n",
        "                                add_special_tokens=True,\n",
        "                                pad_to_max_length=True                     \n",
        "                                )\n",
        "        \n",
        "        for k, v in batch_texts.items():\n",
        "          batch_texts[k] = torch.tensor(v, dtype=torch.long)\n",
        "\n",
        "        batch_y = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "\n",
        "#Model\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self, mpd):\n",
        "        super(MeanPooling, self).__init__()\n",
        "        self.mp_depth = mpd\n",
        "        \n",
        "    def forward(self, last_hidden_state,hidden_states, attention_mask):\n",
        "        mp_embeddings = []\n",
        "\n",
        "        for i in range(self.mp_depth):\n",
        "            if i ==0:\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1) \n",
        "            else:\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states[i-1].size()).float()\n",
        "                sum_embeddings = torch.sum(hidden_states[i-1] * input_mask_expanded, 1)   \n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            \n",
        "            mp_embeddings.append(mean_embeddings)\n",
        "    \n",
        "        results = torch.cat(mp_embeddings , dim=1)\n",
        "        results = results.reshape(results.size(0), self.mp_depth, int(results.size(1)/self.mp_depth))\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "class DBB(nn.Module):\n",
        "    def __init__(self, cfg, mp_depth):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.mp_depth = mp_depth\n",
        "        self.config = AutoConfig.from_pretrained(cfg.model)\n",
        "        self.config.output_hidden_states=True\n",
        "        self.config.hidden_dropout_prob = 0.\n",
        "        self.config.attention_probs_dropout_prob = 0.\n",
        "        self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
        "        self.pool = MeanPooling(self.mp_depth)\n",
        "        self.mpd = nn.Linear(mp_depth, 1)\n",
        "        self.out = nn.Linear(self.config.hidden_size, 6)\n",
        "        self._init_weights(self.mpd)\n",
        "        self._init_weights(self.out)\n",
        "        self._init_weights(self.pool)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "#         last_hidden_state = outputs[0]\n",
        "#         hidden_states = outputs[1]\n",
        "        pooled_outputs = self.pool(outputs.last_hidden_state,outputs.hidden_states,  inputs['attention_mask'])\n",
        "        pooled_outputs = pooled_outputs.permute(0,2,1)\n",
        "        mean_pooled = self.mpd(pooled_outputs)\n",
        "        mean_pooled =mean_pooled.squeeze(-1)\n",
        "        final_out = self.out(mean_pooled)\n",
        "        return final_out\n",
        "\n",
        "# ====================================================\n",
        "#####Loss\n",
        "#====================================================\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, reduction='mean', eps=1e-9):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        self.reduction = reduction\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
        "        if self.reduction == 'none':\n",
        "            loss = loss\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "    \n",
        "def get_lr_groups(model, learning_rate=cfg.lr, layer_decay=cfg.layer_decay):\n",
        "   \n",
        "    n_layers = len(model.model.encoder.layer) + 6 # + 1 (embedding) +2 layernorm.. +2 lin\n",
        "\n",
        "    embedding_decayed_lr = learning_rate * (layer_decay ** (n_layers+6))\n",
        "    grouped_parameters = [{\"params\": model.model.embeddings.parameters(), 'lr': embedding_decayed_lr}]\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    for depth in range(1, n_layers-5):\n",
        "        decayed_lr = learning_rate * (layer_decay ** (n_layers + 6 - depth))\n",
        "        grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.model.encoder.layer[depth-1].named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'lr': decayed_lr, 'weight_decay': cfg.weight_decay}\n",
        "        )\n",
        "        grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.model.encoder.layer[depth-1].named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             'lr': decayed_lr, 'weight_decay': 0.0})\n",
        "        \n",
        "    #rel embeddings layer\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.model.encoder.rel_embeddings.named_parameters() if not any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate * (layer_decay ** 4), 'weight_decay': cfg.weight_decay})\n",
        "    grouped_parameters.append(\n",
        "        {\"params\": [p for n, p in model.model.encoder.rel_embeddings.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "         'lr': learning_rate * (layer_decay ** 4), 'weight_decay': 0.0})\n",
        "    \n",
        "    #layer norm layer\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.model.encoder.LayerNorm.named_parameters() if not any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate * (layer_decay ** 3), 'weight_decay': cfg.weight_decay})\n",
        "    grouped_parameters.append(\n",
        "        {\"params\": [p for n, p in model.model.encoder.LayerNorm.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "         'lr': learning_rate * (layer_decay ** 3), 'weight_decay': 0.0})    \n",
        "    \n",
        "    #Pooling layer\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.pool.named_parameters() if not any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate * (layer_decay ** 2), 'weight_decay': cfg.weight_decay})\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.pool.named_parameters() if any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate * (layer_decay ** 2), 'weight_decay': 0.0})  \n",
        "    \n",
        "\n",
        "    #mpd layer\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.mpd.named_parameters() if not any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate * (layer_decay ** 1), 'weight_decay': cfg.weight_decay})\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.mpd.named_parameters() if any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate * (layer_decay ** 1), 'weight_decay': 0.0})    \n",
        "\n",
        "    #out layer\n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.out.named_parameters() if not any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate, 'weight_decay': cfg.weight_decay }\n",
        "            )     \n",
        "    grouped_parameters.append(\n",
        "            {\"params\": [p for n, p in model.out.named_parameters() if any(nd in n for nd in no_decay)], \n",
        "             'lr': learning_rate, 'weight_decay': 0.0 }\n",
        "            )    \n",
        "       \n",
        "    return grouped_parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_parms(model, lr):\n",
        "\n",
        "    # save layer names\n",
        "    layer_names = []\n",
        "    for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "      layer_names.append(name)\n",
        "    print(f'{idx}: {name}')\n",
        "    layer_names.reverse()\n",
        "\n",
        "\n",
        "    lr      = lr\n",
        "    lr_mult = 0.9\n",
        "\n",
        "    # placeholder\n",
        "    parameters = []\n",
        "\n",
        "    # store params & learning rates\n",
        "    for idx, name in enumerate(layer_names):\n",
        "    \n",
        "      # display info\n",
        "      print(f'{idx}: lr = {lr:.6f}, {name}')\n",
        "      \n",
        "      # append layer parameters\n",
        "      parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
        "                      'lr':     lr}]\n",
        "      \n",
        "      # update learning rate\n",
        "      lr *= lr_mult\n",
        "    return parameters\n",
        "\n",
        "\n",
        "    \n",
        "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "          'lr': encoder_lr, 'weight_decay': weight_decay},\n",
        "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "          'lr': encoder_lr, 'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
        "          'lr': decoder_lr, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    return optimizer_parameters\n",
        "\n",
        "def collate(inputs):\n",
        "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = inputs[k][:,:mask_len]\n",
        "    return inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "vqhigdnNAGYq",
        "execution": {
          "iopub.status.busy": "2022-11-01T19:09:02.122829Z",
          "iopub.execute_input": "2022-11-01T19:09:02.123643Z",
          "iopub.status.idle": "2022-11-01T19:09:02.188951Z",
          "shell.execute_reply.started": "2022-11-01T19:09:02.123601Z",
          "shell.execute_reply": "2022-11-01T19:09:02.187896Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    os.environ['WANDB_NOTEBOOK_NAME'] = '/content/drive/MyDrive/Colab Notebooks/Db-base-2-train.ipynb'\n",
        "\n",
        "    run = wandb.init(project='FB3-Public', \n",
        "                     name=cfg.model,\n",
        "                     config=class2dict(cfg),\n",
        "                     group=cfg.model,\n",
        "                     job_type=\"train\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "    for val_fold in range(cfg.n_fold):\n",
        "        oof_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "        train_folds = df_train[df_train['fold'] != val_fold].reset_index(drop=True)\n",
        "        valid_folds = df_train[df_train['fold'] == val_fold].reset_index(drop=True)\n",
        "        valid_labels = valid_folds[cfg.target_cols].values\n",
        "\n",
        "        train_dataset = Dataset_Db(cfg, train_folds)\n",
        "        valid_dataset = Dataset_Db(cfg, valid_folds)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                  batch_size=cfg.batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=cfg.num_workers, \n",
        "                                  pin_memory=True#, \n",
        "                                  #drop_last=True\n",
        "                                  )\n",
        "        valid_loader = DataLoader(valid_dataset,\n",
        "                                  batch_size=cfg.batch_size * 2,\n",
        "                                  shuffle=False,\n",
        "                                  num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "        model = DBB(cfg,cfg.mp_depth)\n",
        "        #torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
        "        model.to(device)\n",
        "\n",
        "        lr_groups = get_lr_groups(model, learning_rate=1e-2)\n",
        "        #parms = get_parms(model, cfg.lr)\n",
        "        optimizer = AdamW(lr_groups, lr=cfg.lr, eps=cfg.eps, betas=cfg.betas)\n",
        "\n",
        "\n",
        "        # optimizer_parameters = get_optimizer_params(model,\n",
        "        #                                         encoder_lr=cfg.lr, \n",
        "        #                                         decoder_lr=cfg.lr,\n",
        "        #                                         weight_decay=cfg.weight_decay)\n",
        "        # optimizer = AdamW(optimizer_parameters, lr=cfg.lr, eps=cfg.eps, betas=cfg.betas)\n",
        "\n",
        "\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
        "                                                    num_warmup_steps=cfg.num_warmup_steps,\n",
        "                                                    num_training_steps=int(len(train_folds) /cfg.batch_size * cfg.epochs), \n",
        "                                                    num_cycles=cfg.num_cycles\n",
        "                                                    )\n",
        "\n",
        "\n",
        "\n",
        "        criterion = RMSELoss() #RMSELoss(reduction=\"mean\")\n",
        "\n",
        "        best_score = np.inf\n",
        "\n",
        "\n",
        "        for epoch in range(cfg.epochs):\n",
        "\n",
        "            model.train()\n",
        "            scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "            losses = []\n",
        "            counter = 0\n",
        "\n",
        "            #train\n",
        "            for step, (inputs, labels) in enumerate(train_loader):\n",
        "                inputs = collate(inputs)\n",
        "                for k, v in inputs.items():\n",
        "                    inputs[k] = v.to(device)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "                batch_size = labels.size(0)\n",
        "\n",
        "                #with torch.cuda.amp.autocast(enabled=True):\n",
        "                y_preds = model(inputs)\n",
        "                loss = criterion(y_preds, labels)\n",
        "\n",
        "                if cfg.accumulation_steps > 1:\n",
        "                    loss = loss / cfg.accumulation_steps\n",
        "\n",
        "                losses.append(loss*batch_size)\n",
        "                counter += batch_size\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                \n",
        "#                 # before gradient clipping the optimizer parameters must be unscaled.\n",
        "#                 scaler.unscale_(optimizer)\n",
        "    \n",
        "#                 # perform optimization step\n",
        "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_norm)\n",
        "\n",
        "                if (step + 1) % cfg.accumulation_steps == 0 or step == len(train_loader):\n",
        "                    #Gradient Value Clipping\n",
        "                    #nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "\n",
        "                total = sum(losses)/counter\n",
        "\n",
        "                if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n",
        "                    print(f'Epoch: [{epoch}][{step}/{len(train_loader)}]  \\n',\n",
        "                        f'Loss: {total}')\n",
        "                \n",
        "                wandb.log({f\"[fold{val_fold}] loss\": loss*cfg.accumulation_steps,\n",
        "                       f\"[fold{val_fold}] lr\": scheduler.get_lr()[0]})\n",
        "\n",
        "                # Optional\n",
        "                wandb.watch(model)\n",
        "\n",
        "\n",
        "\n",
        "          #validation\n",
        "\n",
        "            val_losses = []\n",
        "            val_counter = 0\n",
        "            preds = []\n",
        "            model.eval()\n",
        "\n",
        "            for step, (inputs, labels) in enumerate(valid_loader):\n",
        "                inputs = collate(inputs)\n",
        "\n",
        "                for k, v in inputs.items():\n",
        "                    inputs[k] = v.to(device)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "                batch_size = labels.size(0)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    val_y_preds = model(inputs)\n",
        "                    val_loss = criterion(val_y_preds, labels)\n",
        "                    \n",
        "                if cfg.accumulation_steps > 1:\n",
        "                    val_loss = val_loss / cfg.accumulation_steps\n",
        "\n",
        "                val_losses.append(val_loss*batch_size)\n",
        "                val_counter += batch_size\n",
        "\n",
        "                total_val = sum(val_losses)/val_counter\n",
        "                preds.append(val_y_preds.to('cpu').numpy())\n",
        "\n",
        "\n",
        "\n",
        "            predictions = np.concatenate(preds)\n",
        "            total_val_loss = sum(val_losses)/val_counter\n",
        "            print(f'***************EVAL: Loss: {total_val_loss}')\n",
        "\n",
        "            wandb.log({f\"[fold{val_fold}] loss\": val_loss*cfg.accumulation_steps})\n",
        "\n",
        "\n",
        "            if best_score > total_val_loss:\n",
        "                best_score = total_val_loss\n",
        "                torch.save({'model': model.state_dict(),\n",
        "                              'predictions': predictions},\n",
        "                              SAVE_PATH+f\"{cfg.model.replace('/', '-')}_fold{val_fold}_m1.pth\")\n",
        "\n",
        "        del model\n"
      ],
      "metadata": {
        "id": "3IGJPW0jF1m4",
        "execution": {
          "iopub.status.busy": "2022-11-01T19:09:02.190636Z",
          "iopub.execute_input": "2022-11-01T19:09:02.191345Z",
          "iopub.status.idle": "2022-11-01T19:09:02.215665Z",
          "shell.execute_reply.started": "2022-11-01T19:09:02.191290Z",
          "shell.execute_reply": "2022-11-01T19:09:02.214587Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test, submission = load_prepare()\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n",
        "#tokenizer.save_pretrained(SAVE_PATH+'tokenizer/')\n",
        "cfg.tokenizer = tokenizer\n",
        "train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-01T19:09:02.217587Z",
          "iopub.execute_input": "2022-11-01T19:09:02.218497Z",
          "iopub.status.idle": "2022-11-01T19:09:50.521768Z",
          "shell.execute_reply.started": "2022-11-01T19:09:02.218399Z",
          "shell.execute_reply": "2022-11-01T19:09:50.519729Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VKe8C6MIQfs2",
        "outputId": "0f3e112f-28de-4a47-9d40-a781beb1c9ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "fold\n",
              "0    978\n",
              "1    977\n",
              "2    978\n",
              "3    978\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33martv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/kaggle Feedback/wandb/run-20221108_101956-25qp2brt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/artv/FB3-Public/runs/25qp2brt\" target=\"_blank\">microsoft/deberta-v3-base</a></strong> to <a href=\"https://wandb.ai/artv/FB3-Public\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/978]  \n",
            " Loss: 0.07746513187885284\n",
            "Epoch: [0][100/978]  \n",
            " Loss: 0.07628502696752548\n",
            "Epoch: [0][200/978]  \n",
            " Loss: 0.07382301986217499\n",
            "Epoch: [0][300/978]  \n",
            " Loss: 0.0701470822095871\n",
            "Epoch: [0][400/978]  \n",
            " Loss: 0.06563375890254974\n",
            "Epoch: [0][500/978]  \n",
            " Loss: 0.0603409968316555\n",
            "Epoch: [0][600/978]  \n",
            " Loss: 0.054493486881256104\n",
            "Epoch: [0][700/978]  \n",
            " Loss: 0.04876348376274109\n",
            "Epoch: [0][800/978]  \n",
            " Loss: 0.044660650193691254\n",
            "Epoch: [0][900/978]  \n",
            " Loss: 0.04165166616439819\n",
            "Epoch: [0][977/978]  \n",
            " Loss: 0.03947523608803749\n",
            "***************EVAL: Loss: 0.012957543134689331\n",
            "Epoch: [1][0/978]  \n",
            " Loss: 0.01554647646844387\n",
            "Epoch: [1][100/978]  \n",
            " Loss: 0.013739284127950668\n",
            "Epoch: [1][200/978]  \n",
            " Loss: 0.014567920938134193\n",
            "Epoch: [1][300/978]  \n",
            " Loss: 0.014154591597616673\n",
            "Epoch: [1][400/978]  \n",
            " Loss: 0.01357951108366251\n",
            "Epoch: [1][500/978]  \n",
            " Loss: 0.013420423492789268\n",
            "Epoch: [1][600/978]  \n",
            " Loss: 0.013410741463303566\n",
            "Epoch: [1][700/978]  \n",
            " Loss: 0.013442550785839558\n",
            "Epoch: [1][800/978]  \n",
            " Loss: 0.013271472416818142\n",
            "Epoch: [1][900/978]  \n",
            " Loss: 0.013181670568883419\n",
            "Epoch: [1][977/978]  \n",
            " Loss: 0.013079427182674408\n",
            "***************EVAL: Loss: 0.01185990497469902\n",
            "Epoch: [2][0/978]  \n",
            " Loss: 0.010964005254209042\n",
            "Epoch: [2][100/978]  \n",
            " Loss: 0.01185530237853527\n",
            "Epoch: [2][200/978]  \n",
            " Loss: 0.011774295009672642\n",
            "Epoch: [2][300/978]  \n",
            " Loss: 0.011660587973892689\n",
            "Epoch: [2][400/978]  \n",
            " Loss: 0.01165514811873436\n",
            "Epoch: [2][500/978]  \n",
            " Loss: 0.011612437665462494\n",
            "Epoch: [2][600/978]  \n",
            " Loss: 0.01154340710490942\n",
            "Epoch: [2][700/978]  \n",
            " Loss: 0.011455775238573551\n",
            "Epoch: [2][800/978]  \n",
            " Loss: 0.011331014335155487\n",
            "Epoch: [2][900/978]  \n",
            " Loss: 0.011273197829723358\n",
            "Epoch: [2][977/978]  \n",
            " Loss: 0.011367560364305973\n",
            "***************EVAL: Loss: 0.01190240029245615\n",
            "Epoch: [3][0/978]  \n",
            " Loss: 0.01589144766330719\n",
            "Epoch: [3][100/978]  \n",
            " Loss: 0.01072588749229908\n",
            "Epoch: [3][200/978]  \n",
            " Loss: 0.010615615174174309\n",
            "Epoch: [3][300/978]  \n",
            " Loss: 0.010673894546926022\n",
            "Epoch: [3][400/978]  \n",
            " Loss: 0.010643374174833298\n",
            "Epoch: [3][500/978]  \n",
            " Loss: 0.010529903694987297\n",
            "Epoch: [3][600/978]  \n",
            " Loss: 0.010482244193553925\n",
            "Epoch: [3][700/978]  \n",
            " Loss: 0.010545225813984871\n",
            "Epoch: [3][800/978]  \n",
            " Loss: 0.010538773611187935\n",
            "Epoch: [3][900/978]  \n",
            " Loss: 0.010583888739347458\n",
            "Epoch: [3][977/978]  \n",
            " Loss: 0.010561498813331127\n",
            "***************EVAL: Loss: 0.010562773793935776\n",
            "Epoch: [4][0/978]  \n",
            " Loss: 0.008499518036842346\n",
            "Epoch: [4][100/978]  \n",
            " Loss: 0.010266430675983429\n",
            "Epoch: [4][200/978]  \n",
            " Loss: 0.010291986167430878\n",
            "Epoch: [4][300/978]  \n",
            " Loss: 0.010314957238733768\n",
            "Epoch: [4][400/978]  \n",
            " Loss: 0.01031258050352335\n",
            "Epoch: [4][500/978]  \n",
            " Loss: 0.010339283384382725\n",
            "Epoch: [4][600/978]  \n",
            " Loss: 0.010332482866942883\n",
            "Epoch: [4][700/978]  \n",
            " Loss: 0.010424676351249218\n",
            "Epoch: [4][800/978]  \n",
            " Loss: 0.010369276627898216\n",
            "Epoch: [4][900/978]  \n",
            " Loss: 0.010353080928325653\n",
            "Epoch: [4][977/978]  \n",
            " Loss: 0.010385769419372082\n",
            "***************EVAL: Loss: 0.010587475262582302\n",
            "Epoch: [5][0/978]  \n",
            " Loss: 0.010053243488073349\n",
            "Epoch: [5][100/978]  \n",
            " Loss: 0.010341882705688477\n",
            "Epoch: [5][200/978]  \n",
            " Loss: 0.01029196660965681\n",
            "Epoch: [5][300/978]  \n",
            " Loss: 0.010465268976986408\n",
            "Epoch: [5][400/978]  \n",
            " Loss: 0.010515484027564526\n",
            "Epoch: [5][500/978]  \n",
            " Loss: 0.010556874796748161\n",
            "Epoch: [5][600/978]  \n",
            " Loss: 0.010581344366073608\n",
            "Epoch: [5][700/978]  \n",
            " Loss: 0.0105329854413867\n",
            "Epoch: [5][800/978]  \n",
            " Loss: 0.010494505986571312\n",
            "Epoch: [5][900/978]  \n",
            " Loss: 0.010467397049069405\n",
            "Epoch: [5][977/978]  \n",
            " Loss: 0.010453862138092518\n",
            "***************EVAL: Loss: 0.010827617719769478\n",
            "Epoch: [6][0/978]  \n",
            " Loss: 0.010666805319488049\n",
            "Epoch: [6][100/978]  \n",
            " Loss: 0.010610214434564114\n",
            "Epoch: [6][200/978]  \n",
            " Loss: 0.010736644268035889\n",
            "Epoch: [6][300/978]  \n",
            " Loss: 0.010680495761334896\n",
            "Epoch: [6][400/978]  \n",
            " Loss: 0.01067751832306385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-9-eeb6e8f91bf7>\", line 5, in <module>\n",
            "    train()\n",
            "  File \"<ipython-input-8-744c9aa93c0e>\", line 97, in train\n",
            "    scaler.scale(loss).backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/wandb_torch.py\", line 282, in <lambda>\n",
            "    handle = var.register_hook(lambda grad: _callback(grad, log_track))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/wandb_torch.py\", line 280, in _callback\n",
            "    self.log_tensor_stats(grad.data, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/wandb_torch.py\", line 258, in log_tensor_stats\n",
            "    commit=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 1391, in _log\n",
            "    self._partial_history_callback(data, step, commit)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_run.py\", line 1280, in _partial_history_callback\n",
            "    publish_step=not_using_tensorboard,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 560, in publish_partial_history\n",
            "    self._publish_partial_history(partial_history)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\", line 85, in _publish_partial_history\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'BrokenPipeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in callback <function _WandbInit._pause_backend at 0x7f972a174d40> (for post_run_cell):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved code: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resume_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mpause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface_sock.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_record_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMessageFuture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mserver_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_publish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_packet_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<BI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendall_with_error_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# sent equal to 0 indicates a closed socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "WFmmXDfUtm4z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}