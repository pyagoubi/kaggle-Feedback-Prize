{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/pyagoubi/kaggle-Feedback-Prize/blob/main/Debertav3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"#Mounting Google Drive\n#from google.colab import drive\n#drive.mount('/content/drive')\n\n#import os\n#os.chdir('/content/drive/MyDrive/kaggle Feedback')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuH7idBliYP8","outputId":"fa563cce-d351-415f-eb19-e01b01264dce","execution":{"iopub.status.busy":"2022-10-09T20:20:18.113889Z","iopub.execute_input":"2022-10-09T20:20:18.114347Z","iopub.status.idle":"2022-10-09T20:20:18.121505Z","shell.execute_reply.started":"2022-10-09T20:20:18.114259Z","shell.execute_reply":"2022-10-09T20:20:18.120555Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"SAVE_PATH = './'\nTRAIN_PATH = '../input/feedback-prize-english-language-learning/train.csv'\nTEST_PATH = '../input/feedback-prize-english-language-learning/test.csv'\nSAMPLE_SUB_PATH = '../input/feedback-prize-english-language-learning/sample_submission.csv' \n\nTARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"id":"sBcRQkP8jGL2","execution":{"iopub.status.busy":"2022-10-09T20:20:18.123754Z","iopub.execute_input":"2022-10-09T20:20:18.124590Z","iopub.status.idle":"2022-10-09T20:20:18.136202Z","shell.execute_reply.started":"2022-10-09T20:20:18.124553Z","shell.execute_reply":"2022-10-09T20:20:18.135273Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install iterative-stratification\n!pip install sentencepiece\n!pip install transformers==4.21.2\n#!pip install iterative-stratification --no-index --find-links=file:../input/iterstratification/iterstrat\n\nimport warnings\nimport sentencepiece\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000) \nfrom tqdm import tqdm\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom torch import autocast\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, BertModel, BertTokenizer\n\nprint('Transformer Version: ', transformers.__version__)","metadata":{"id":"KxsDW5bmDL3v","execution":{"iopub.status.busy":"2022-10-09T20:20:18.139329Z","iopub.execute_input":"2022-10-09T20:20:18.139687Z","iopub.status.idle":"2022-10-09T20:20:47.362842Z","shell.execute_reply.started":"2022-10-09T20:20:18.139652Z","shell.execute_reply":"2022-10-09T20:20:47.361524Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    model= 'microsoft/deberta-v3-base'\n    gradient_checkpointing=True\n    epochs=10\n    eps=1e-6\n    num_workers=4\n    batch_size=10\n    weight_decay=0.01\n    target_cols=TARGET_COLS\n    seed=42\n    n_fold=4\n    train=True\n    #scheduler='cosine' # ['linear', 'cosine']\n    #batch_scheduler=True\n    #num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    print_freq = 100\n    #max_len=512\n\n\n\n\n\ndef import_data(tr =TRAIN_PATH, te =TEST_PATH, sample =SAMPLE_SUB_PATH ):\n  df_train = pd.read_csv(tr)\n  df_test = pd.read_csv(te)\n  submission = pd.read_csv(sample)\n  return df_train, df_test, submission\n\ndef replace_nl(df_train, df_test):\n  df_train['full_text'] = df_train['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl= r'', regex=True)\n  df_test['full_text'] = df_test['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl=r'', regex=True)\n  return df_train, df_test\n\ndef set_folds(df_train):\n  Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n  for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[cfg.target_cols])):\n      df_train.loc[val_index, 'fold'] = int(n)\n  df_train['fold'] = df_train['fold'].astype(int)\n  display(df_train.groupby('fold').size())\n  return df_train\n\ndef load_prepare():\n  df_train, df_test, submission = import_data()\n  df_train, df_test = replace_nl(df_train, df_test)\n  df_train=  set_folds(df_train)\n  return df_train, df_test, submission\n\n","metadata":{"id":"JpNOcDU65l2q","execution":{"iopub.status.busy":"2022-10-09T20:20:47.394036Z","iopub.execute_input":"2022-10-09T20:20:47.395012Z","iopub.status.idle":"2022-10-09T20:20:47.406936Z","shell.execute_reply.started":"2022-10-09T20:20:47.394974Z","shell.execute_reply":"2022-10-09T20:20:47.405846Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train, df_test, submission = load_prepare()","metadata":{"id":"oi4-ULuW8-7B","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"556065df-4d40-420c-8cdd-6b804c5282cc","execution":{"iopub.status.busy":"2022-10-09T20:20:47.408358Z","iopub.execute_input":"2022-10-09T20:20:47.409041Z","iopub.status.idle":"2022-10-09T20:20:47.779987Z","shell.execute_reply.started":"2022-10-09T20:20:47.409004Z","shell.execute_reply":"2022-10-09T20:20:47.778956Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"fold\n0    978\n1    977\n2    978\n3    978\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#Dataset Deberta Base\nclass Dataset_Db(torch.utils.data.Dataset):\n\n    def __init__(self, df, target, train = True):\n        self.train = train\n        self.target = target\n        if self.train: self.labels = df[self.target].values\n        self.texts = df[[\"full_text\"]].values\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        batch_texts = self.tokenizer(self.texts[idx][0], \n                                padding='max_length', \n                                max_length = 512, truncation=True#, \n                                #return_tensors=\"pt\"\n                                )\n        \n        for k, v in batch_texts.items():\n          batch_texts[k] = torch.tensor(v, dtype=torch.long)\n\n        if self.train: batch_y = torch.tensor(self.labels[idx], dtype=torch.float)\n        if self.train: return batch_texts, batch_y\n        else: return batch_texts\n\n#Model Deberta Base\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass DBB(nn.Module):\n  def __init__(self, cfg):\n    super().__init__()\n    self.cfg = cfg\n    self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n    self.model = AutoModel.from_pretrained(cfg.model)\n    self.pool = MeanPooling()\n    # self.linear = nn.Linear(self.config.hidden_size, 512)\n    # self.dropout = nn.Dropout(p=0.1)\n    # self.relu = nn.ReLU()\n    self.out = nn.Linear(self.config.hidden_size, 1)\n    self._init_weights(self.out)\n\n  def _init_weights(self, module):\n      if isinstance(module, nn.Linear):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.bias is not None:\n              module.bias.data.zero_()\n      elif isinstance(module, nn.Embedding):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.padding_idx is not None:\n              module.weight.data[module.padding_idx].zero_()\n      elif isinstance(module, nn.LayerNorm):\n          module.bias.data.zero_()\n          module.weight.data.fill_(1.0)\n\n  def forward(self, inputs):\n      outputs = self.model(**inputs)\n      last_hidden_states = outputs[0]\n      pooled_output = self.pool(last_hidden_states, inputs['attention_mask'])\n      final_out = self.out(pooled_output)\n      return final_out\n    \n# ====================================================\n# Loss\n# ====================================================\n# class RMSELoss(nn.Module):\n#     def __init__(self, reduction='mean', eps=1e-9):\n#         super().__init__()\n#         self.mse = nn.MSELoss(reduction='none')\n#         self.reduction = reduction\n#         self.eps = eps\n\n#     def forward(self, y_pred, y_true):\n#         loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n#         if self.reduction == 'none':\n#             loss = loss\n#         elif self.reduction == 'sum':\n#             loss = loss.sum()\n#         elif self.reduction == 'mean':\n#             loss = loss.mean()\n#         return loss\n\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n          'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\n","metadata":{"id":"vqhigdnNAGYq","execution":{"iopub.status.busy":"2022-10-09T20:20:47.781669Z","iopub.execute_input":"2022-10-09T20:20:47.782108Z","iopub.status.idle":"2022-10-09T20:20:47.800917Z","shell.execute_reply.started":"2022-10-09T20:20:47.782067Z","shell.execute_reply":"2022-10-09T20:20:47.799821Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"LU84H01xiSZc"}},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler()\n\nfor target in cfg.target_cols:\n  for val_fold in range(cfg.n_fold):\n    oof_df = pd.DataFrame()\n\n\n    train_folds = df_train[df_train['fold'] != val_fold].reset_index(drop=True)\n    valid_folds = df_train[df_train['fold'] == val_fold].reset_index(drop=True)\n    valid_labels = valid_folds[target].values\n      \n    train_dataset = Dataset_Db(train_folds, target)\n    valid_dataset = Dataset_Db(valid_folds, target)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=cfg.batch_size,\n                              shuffle=True,\n                              num_workers=cfg.num_workers, \n                              pin_memory=True#, \n                              #drop_last=True\n                              )\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=cfg.batch_size * 2,\n                              shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n    \n    model = DBB(cfg)\n    #torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=cfg.encoder_lr, \n                                                decoder_lr=cfg.decoder_lr,\n                                                weight_decay=cfg.weight_decay)\n    \n    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n\n    criterion = nn.MSELoss() #RMSELoss(reduction=\"mean\")\n\n    best_score = np.inf\n\n    for epoch in range(cfg.epochs):\n      \n      model.train()\n      scaler = torch.cuda.amp.GradScaler(enabled=True)\n\n      losses = []\n      counter = 0\n\n      #train\n      for step, (inputs, labels) in enumerate(train_loader):\n        \n\n\n          for k, v in inputs.items():\n              inputs[k] = v.to(device)\n\n          labels = labels.to(device)\n          batch_size = labels.size(0)\n\n          with torch.cuda.amp.autocast(enabled=True):\n              y_preds = model(inputs)\n              loss = criterion(y_preds, labels)\n\n          losses.append(loss*batch_size)\n          counter += batch_size\n\n          scaler.scale(loss).backward()\n          scaler.step(optimizer)\n          scaler.update()\n          optimizer.zero_grad()\n          total = sum(losses)/counter\n\n          if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n              print(f'Epoch: [{epoch}][{step}/{len(train_loader)}]  \\n',\n                    f'Loss: {total}')\n          \n          \n      #validation\n\n      val_losses = []\n      val_counter = 0\n      preds = []\n      model.eval()\n\n      for step, (inputs, labels) in enumerate(valid_loader):\n\n        for k, v in inputs.items():\n              inputs[k] = v.to(device)\n\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n\n        with torch.no_grad():\n            val_y_preds = model(inputs)\n            val_loss = criterion(val_y_preds, labels)\n\n        val_losses.append(val_loss*batch_size)\n        val_counter += batch_size\n\n        total_val = sum(val_losses)/val_counter\n        preds.append(val_y_preds.to('cpu').numpy())\n\n        if step % cfg.print_freq == 0 or step == (len(valid_loader)-1):\n            print(f'EVAL: [{step}/{len(valid_loader)}] \\n',\n                  f'Loss: {total_val})')\n            \n\n    predictions = np.concatenate(preds)\n    total_val_loss = sum(val_losses)/val_counter\n\n    if best_score > total_val_loss:\n      best_score = total_val_loss\n      torch.save({'model': model.state_dict(),\n                  'predictions': predictions},\n                  SAVE_PATH+f\"{cfg.model.replace('/', '-')}_t{target}_fold{val_fold}_best.pth\")\n      \n    del model\n  \n\n\n\n\n\n\n\n\n\n\n\n\n        ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"3IGJPW0jF1m4","outputId":"fa2a0771-7593-428e-dfcc-8fdedfdc2a47","execution":{"iopub.status.busy":"2022-10-09T21:08:10.808440Z","iopub.execute_input":"2022-10-09T21:08:10.808878Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: [0][0/294]  \n Loss: 11.539921760559082\nEpoch: [0][100/294]  \n Loss: 1.5639533996582031\nEpoch: [0][200/294]  \n Loss: 1.0114071369171143\nEpoch: [0][293/294]  \n Loss: 0.8415274620056152\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.670985758304596)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.424772173166275)\nEpoch: [1][0/294]  \n Loss: 1.0810316801071167\nEpoch: [1][100/294]  \n Loss: 0.4581890404224396\nEpoch: [1][200/294]  \n Loss: 0.4594479203224182\nEpoch: [1][293/294]  \n Loss: 0.4549139738082886\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.6415645480155945)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.42215171456336975)\nEpoch: [2][0/294]  \n Loss: 0.3617004454135895\nEpoch: [2][100/294]  \n Loss: 0.4643958508968353\nEpoch: [2][200/294]  \n Loss: 0.4579312205314636\nEpoch: [2][293/294]  \n Loss: 0.45909759402275085\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.7224829196929932)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.45190244913101196)\nEpoch: [3][0/294]  \n Loss: 0.35446497797966003\nEpoch: [3][100/294]  \n Loss: 0.47405004501342773\nEpoch: [3][200/294]  \n Loss: 0.44822007417678833\nEpoch: [3][293/294]  \n Loss: 0.4535614848136902\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.7144852876663208)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.44685572385787964)\nEpoch: [0][0/294]  \n Loss: 11.567486763000488\nEpoch: [0][100/294]  \n Loss: 1.4650112390518188\nEpoch: [0][200/294]  \n Loss: 0.9590647220611572\nEpoch: [0][293/294]  \n Loss: 0.7985196709632874\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40400591492652893)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4680282473564148)\nEpoch: [1][0/294]  \n Loss: 0.09314132481813431\nEpoch: [1][100/294]  \n Loss: 0.4583982229232788\nEpoch: [1][200/294]  \n Loss: 0.4681895077228546\nEpoch: [1][293/294]  \n Loss: 0.4641820788383484\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3535389006137848)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4362499713897705)\nEpoch: [2][0/294]  \n Loss: 0.47445836663246155\nEpoch: [2][100/294]  \n Loss: 0.45656171441078186\nEpoch: [2][200/294]  \n Loss: 0.44093164801597595\nEpoch: [2][293/294]  \n Loss: 0.453911155462265\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.41421064734458923)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.47498324513435364)\nEpoch: [3][0/294]  \n Loss: 0.6119949221611023\nEpoch: [3][100/294]  \n Loss: 0.42523354291915894\nEpoch: [3][200/294]  \n Loss: 0.4160122275352478\nEpoch: [3][293/294]  \n Loss: 0.44653934240341187\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.41347452998161316)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4741240441799164)\nEpoch: [0][0/294]  \n Loss: 7.699479103088379\nEpoch: [0][100/294]  \n Loss: 1.2677823305130005\nEpoch: [0][200/294]  \n Loss: 0.8448551893234253\nEpoch: [0][293/294]  \n Loss: 0.7284319996833801\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3788259029388428)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4775742292404175)\nEpoch: [1][0/294]  \n Loss: 0.35564491152763367\nEpoch: [1][100/294]  \n Loss: 0.4358447194099426\nEpoch: [1][200/294]  \n Loss: 0.4465639591217041\nEpoch: [1][293/294]  \n Loss: 0.4482586681842804\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.344949334859848)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4539639949798584)\nEpoch: [2][0/294]  \n Loss: 0.6836816072463989\nEpoch: [2][100/294]  \n Loss: 0.4404338002204895\nEpoch: [2][200/294]  \n Loss: 0.4339538812637329\nEpoch: [2][293/294]  \n Loss: 0.4407350718975067\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3471836745738983)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4551706612110138)\nEpoch: [3][0/294]  \n Loss: 0.5064175128936768\nEpoch: [3][100/294]  \n Loss: 0.4158453643321991\nEpoch: [3][200/294]  \n Loss: 0.44084399938583374\nEpoch: [3][293/294]  \n Loss: 0.4453691840171814\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4266444146633148)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5168712139129639)\nEpoch: [0][0/294]  \n Loss: 10.649890899658203\nEpoch: [0][100/294]  \n Loss: 1.4482874870300293\nEpoch: [0][200/294]  \n Loss: 0.9485348463058472\nEpoch: [0][293/294]  \n Loss: 0.7998358011245728\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4144146144390106)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4596903920173645)\nEpoch: [1][0/294]  \n Loss: 0.7124607563018799\nEpoch: [1][100/294]  \n Loss: 0.46297401189804077\nEpoch: [1][200/294]  \n Loss: 0.4378795027732849\nEpoch: [1][293/294]  \n Loss: 0.44721078872680664\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4495488703250885)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5261949300765991)\nEpoch: [2][0/294]  \n Loss: 0.9644545912742615\nEpoch: [2][100/294]  \n Loss: 0.44553714990615845\nEpoch: [2][200/294]  \n Loss: 0.4541035592556\nEpoch: [2][293/294]  \n Loss: 0.4500766396522522\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4138390123844147)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.45372122526168823)\nEpoch: [3][0/294]  \n Loss: 0.3241635859012604\nEpoch: [3][100/294]  \n Loss: 0.45399871468544006\nEpoch: [3][200/294]  \n Loss: 0.4556470215320587\nEpoch: [3][293/294]  \n Loss: 0.44869524240493774\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4120516777038574)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4546668231487274)\nEpoch: [0][0/294]  \n Loss: 5.27944803237915\nEpoch: [0][100/294]  \n Loss: 0.8988694548606873\nEpoch: [0][200/294]  \n Loss: 0.6761114001274109\nEpoch: [0][293/294]  \n Loss: 0.6015175580978394\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.7153770327568054)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5197904706001282)\nEpoch: [1][0/294]  \n Loss: 0.5401695370674133\nEpoch: [1][100/294]  \n Loss: 0.4117887616157532\nEpoch: [1][200/294]  \n Loss: 0.42533212900161743\nEpoch: [1][293/294]  \n Loss: 0.4339287281036377\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5835956931114197)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4051116704940796)\nEpoch: [2][0/294]  \n Loss: 0.6642317771911621\nEpoch: [2][100/294]  \n Loss: 0.4403466582298279\nEpoch: [2][200/294]  \n Loss: 0.42859798669815063\nEpoch: [2][293/294]  \n Loss: 0.42503681778907776\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5764204263687134)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40343785285949707)\nEpoch: [3][0/294]  \n Loss: 0.27519646286964417\nEpoch: [3][100/294]  \n Loss: 0.4594739079475403\nEpoch: [3][200/294]  \n Loss: 0.4383978247642517\nEpoch: [3][293/294]  \n Loss: 0.42831340432167053\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.6288618445396423)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4432917535305023)\nEpoch: [0][0/294]  \n Loss: 17.376699447631836\nEpoch: [0][100/294]  \n Loss: 2.013767957687378\nEpoch: [0][200/294]  \n Loss: 1.2328224182128906\nEpoch: [0][293/294]  \n Loss: 0.9810991883277893\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5712609887123108)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.48617053031921387)\nEpoch: [1][0/294]  \n Loss: 0.49404269456863403\nEpoch: [1][100/294]  \n Loss: 0.4734177887439728\nEpoch: [1][200/294]  \n Loss: 0.43531495332717896\nEpoch: [1][293/294]  \n Loss: 0.43093568086624146\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4855073392391205)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4243790805339813)\nEpoch: [2][0/294]  \n Loss: 0.8697571158409119\nEpoch: [2][100/294]  \n Loss: 0.4314206838607788\nEpoch: [2][200/294]  \n Loss: 0.42315658926963806\nEpoch: [2][293/294]  \n Loss: 0.42652204632759094\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4737168848514557)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.41727763414382935)\nEpoch: [3][0/294]  \n Loss: 0.42370501160621643\nEpoch: [3][100/294]  \n Loss: 0.4164126515388489\nEpoch: [3][200/294]  \n Loss: 0.4257902503013611\nEpoch: [3][293/294]  \n Loss: 0.42790302634239197\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4954051077365875)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4304383099079132)\nEpoch: [0][0/294]  \n Loss: 16.849695205688477\nEpoch: [0][100/294]  \n Loss: 1.8016036748886108\nEpoch: [0][200/294]  \n Loss: 1.1267088651657104\nEpoch: [0][293/294]  \n Loss: 0.9056110978126526\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.38040152192115784)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.519230306148529)\nEpoch: [1][0/294]  \n Loss: 0.19087471067905426\nEpoch: [1][100/294]  \n Loss: 0.4153009057044983\nEpoch: [1][200/294]  \n Loss: 0.41936105489730835\nEpoch: [1][293/294]  \n Loss: 0.42018064856529236\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3088626563549042)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4294767379760742)\nEpoch: [2][0/294]  \n Loss: 0.5683199763298035\nEpoch: [2][100/294]  \n Loss: 0.441237211227417\nEpoch: [2][200/294]  \n Loss: 0.4250570833683014\nEpoch: [2][293/294]  \n Loss: 0.42207813262939453\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.36581841111183167)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4631893038749695)\nEpoch: [3][0/294]  \n Loss: 0.5279203057289124\nEpoch: [3][100/294]  \n Loss: 0.44618070125579834\nEpoch: [3][200/294]  \n Loss: 0.43306127190589905\nEpoch: [3][293/294]  \n Loss: 0.43182361125946045\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3073790669441223)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4246599078178406)\nEpoch: [0][0/294]  \n Loss: 8.76893138885498\nEpoch: [0][100/294]  \n Loss: 1.3046222925186157\nEpoch: [0][200/294]  \n Loss: 0.8669847249984741\nEpoch: [0][293/294]  \n Loss: 0.7280502319335938\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.7938586473464966)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5704039335250854)\nEpoch: [1][0/294]  \n Loss: 0.7685633897781372\nEpoch: [1][100/294]  \n Loss: 0.43259042501449585\nEpoch: [1][200/294]  \n Loss: 0.4196605384349823\nEpoch: [1][293/294]  \n Loss: 0.42935967445373535\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.641291618347168)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4411456882953644)\nEpoch: [2][0/294]  \n Loss: 0.478048712015152\nEpoch: [2][100/294]  \n Loss: 0.4388788342475891\nEpoch: [2][200/294]  \n Loss: 0.4328162372112274\nEpoch: [2][293/294]  \n Loss: 0.42391398549079895\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.61830735206604)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4261118173599243)\nEpoch: [3][0/294]  \n Loss: 0.3748645782470703\nEpoch: [3][100/294]  \n Loss: 0.4268861711025238\nEpoch: [3][200/294]  \n Loss: 0.42590785026550293\nEpoch: [3][293/294]  \n Loss: 0.4226393401622772\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.6470780968666077)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.44552433490753174)\nEpoch: [0][0/294]  \n Loss: 11.03513240814209\nEpoch: [0][100/294]  \n Loss: 1.2935231924057007\nEpoch: [0][200/294]  \n Loss: 0.8224402070045471\nEpoch: [0][293/294]  \n Loss: 0.6801307797431946\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5446417331695557)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3354463577270508)\nEpoch: [1][0/294]  \n Loss: 0.09410613775253296\nEpoch: [1][100/294]  \n Loss: 0.3619422912597656\nEpoch: [1][200/294]  \n Loss: 0.3498985171318054\nEpoch: [1][293/294]  \n Loss: 0.35638681054115295\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5417237877845764)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3334418535232544)\nEpoch: [2][0/294]  \n Loss: 0.6390067934989929\nEpoch: [2][100/294]  \n Loss: 0.3371271789073944\nEpoch: [2][200/294]  \n Loss: 0.35341039299964905\nEpoch: [2][293/294]  \n Loss: 0.35797828435897827\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.6431489586830139)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40680524706840515)\nEpoch: [3][0/294]  \n Loss: 0.4034961760044098\nEpoch: [3][100/294]  \n Loss: 0.3497634530067444\nEpoch: [3][200/294]  \n Loss: 0.3679977059364319\nEpoch: [3][293/294]  \n Loss: 0.35509636998176575\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5680772066116333)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.35087406635284424)\nEpoch: [0][0/294]  \n Loss: 15.648457527160645\nEpoch: [0][100/294]  \n Loss: 2.2287516593933105\nEpoch: [0][200/294]  \n Loss: 1.2814841270446777\nEpoch: [0][293/294]  \n Loss: 0.9939425587654114\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3617195188999176)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.34554561972618103)\nEpoch: [1][0/294]  \n Loss: 0.3067153990268707\nEpoch: [1][100/294]  \n Loss: 0.35335344076156616\nEpoch: [1][200/294]  \n Loss: 0.3488480746746063\nEpoch: [1][293/294]  \n Loss: 0.34925922751426697\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3607330918312073)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3503085970878601)\nEpoch: [2][0/294]  \n Loss: 0.13297699391841888\nEpoch: [2][100/294]  \n Loss: 0.3596452474594116\nEpoch: [2][200/294]  \n Loss: 0.3594677150249481\nEpoch: [2][293/294]  \n Loss: 0.34991398453712463\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3988697826862335)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.411991149187088)\nEpoch: [3][0/294]  \n Loss: 0.2878551483154297\nEpoch: [3][100/294]  \n Loss: 0.3342803418636322\nEpoch: [3][200/294]  \n Loss: 0.34999704360961914\nEpoch: [3][293/294]  \n Loss: 0.35323113203048706\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40643149614334106)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4218251705169678)\nEpoch: [0][0/294]  \n Loss: 7.729785442352295\nEpoch: [0][100/294]  \n Loss: 1.1555135250091553\nEpoch: [0][200/294]  \n Loss: 0.7559931874275208\nEpoch: [0][293/294]  \n Loss: 0.6220495104789734\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4011234939098358)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40764033794403076)\nEpoch: [1][0/294]  \n Loss: 0.35689055919647217\nEpoch: [1][100/294]  \n Loss: 0.35117581486701965\nEpoch: [1][200/294]  \n Loss: 0.3408697545528412\nEpoch: [1][293/294]  \n Loss: 0.34115439653396606\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4179745316505432)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.42013680934906006)\nEpoch: [2][0/294]  \n Loss: 0.3780524432659149\nEpoch: [2][100/294]  \n Loss: 0.3391178846359253\nEpoch: [2][200/294]  \n Loss: 0.34104621410369873\nEpoch: [2][293/294]  \n Loss: 0.34787121415138245\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3998027741909027)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4076730012893677)\nEpoch: [3][0/294]  \n Loss: 0.21945343911647797\nEpoch: [3][100/294]  \n Loss: 0.3279857337474823\nEpoch: [3][200/294]  \n Loss: 0.34597304463386536\nEpoch: [3][293/294]  \n Loss: 0.3456578850746155\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.30999401211738586)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.36466509103775024)\nEpoch: [0][0/294]  \n Loss: 7.60106897354126\nEpoch: [0][100/294]  \n Loss: 1.1831247806549072\nEpoch: [0][200/294]  \n Loss: 0.7688027620315552\nEpoch: [0][293/294]  \n Loss: 0.6456091403961182\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4618014395236969)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.37850236892700195)\nEpoch: [1][0/294]  \n Loss: 0.5063416957855225\nEpoch: [1][100/294]  \n Loss: 0.3521406352519989\nEpoch: [1][200/294]  \n Loss: 0.36611929535865784\nEpoch: [1][293/294]  \n Loss: 0.35139596462249756\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4663666784763336)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3839830458164215)\nEpoch: [2][0/294]  \n Loss: 0.3748238980770111\nEpoch: [2][100/294]  \n Loss: 0.34570977091789246\nEpoch: [2][200/294]  \n Loss: 0.35627585649490356\nEpoch: [2][293/294]  \n Loss: 0.3461054861545563\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.43936577439308167)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.343953013420105)\nEpoch: [3][0/294]  \n Loss: 0.521026074886322\nEpoch: [3][100/294]  \n Loss: 0.329963356256485\nEpoch: [3][200/294]  \n Loss: 0.339199960231781\nEpoch: [3][293/294]  \n Loss: 0.34757328033447266\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5395472645759583)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4652119576931)\nEpoch: [0][0/294]  \n Loss: 11.033782005310059\nEpoch: [0][100/294]  \n Loss: 1.7045880556106567\nEpoch: [0][200/294]  \n Loss: 1.088365912437439\nEpoch: [0][293/294]  \n Loss: 0.8760936260223389\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.6160643100738525)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4732402563095093)\nEpoch: [1][0/294]  \n Loss: 0.1271211951971054\nEpoch: [1][100/294]  \n Loss: 0.4338102638721466\nEpoch: [1][200/294]  \n Loss: 0.4377730190753937\nEpoch: [1][293/294]  \n Loss: 0.4399262070655823\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.519504964351654)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4286805987358093)\nEpoch: [2][0/294]  \n Loss: 0.2941138446331024\nEpoch: [2][100/294]  \n Loss: 0.4123486876487732\nEpoch: [2][200/294]  \n Loss: 0.43734708428382874\nEpoch: [2][293/294]  \n Loss: 0.440194696187973\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5620601773262024)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4441116154193878)\nEpoch: [3][0/294]  \n Loss: 0.49157577753067017\nEpoch: [3][100/294]  \n Loss: 0.418267160654068\nEpoch: [3][200/294]  \n Loss: 0.43846818804740906\nEpoch: [3][293/294]  \n Loss: 0.4377596080303192\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5477538108825684)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.43833839893341064)\nEpoch: [0][0/294]  \n Loss: 6.312557220458984\nEpoch: [0][100/294]  \n Loss: 0.908226728439331\nEpoch: [0][200/294]  \n Loss: 0.674138605594635\nEpoch: [0][293/294]  \n Loss: 0.6135451197624207\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3906422257423401)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.4525241255760193)\nEpoch: [1][0/294]  \n Loss: 0.24204979836940765\nEpoch: [1][100/294]  \n Loss: 0.46228399872779846\nEpoch: [1][200/294]  \n Loss: 0.45520511269569397\nEpoch: [1][293/294]  \n Loss: 0.44746679067611694\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.36179283261299133)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.41870343685150146)\nEpoch: [2][0/294]  \n Loss: 0.5787424445152283\nEpoch: [2][100/294]  \n Loss: 0.43827569484710693\nEpoch: [2][200/294]  \n Loss: 0.44230377674102783\nEpoch: [2][293/294]  \n Loss: 0.45215216279029846\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3590085506439209)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40646347403526306)\nEpoch: [3][0/294]  \n Loss: 0.36104241013526917\nEpoch: [3][100/294]  \n Loss: 0.45301368832588196\nEpoch: [3][200/294]  \n Loss: 0.4489313066005707\nEpoch: [3][293/294]  \n Loss: 0.4478442966938019\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.3620671033859253)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.40818676352500916)\nEpoch: [0][0/294]  \n Loss: 6.729805946350098\nEpoch: [0][100/294]  \n Loss: 1.1809940338134766\nEpoch: [0][200/294]  \n Loss: 0.8129034042358398\nEpoch: [0][293/294]  \n Loss: 0.704673707485199\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.48720046877861023)\nEVAL: [{step}/{len(valid_loader)}] \n Loss: 0.5401865839958191)\nEpoch: [1][0/294]  \n Loss: 0.43393516540527344\nEpoch: [1][100/294]  \n Loss: 0.45562687516212463\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"nVgpoeUSPfCh"},"execution_count":null,"outputs":[]}]}