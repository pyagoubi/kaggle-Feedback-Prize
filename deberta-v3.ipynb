{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/pyagoubi/kaggle-Feedback-Prize/blob/main/Debertav3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"#Mounting Google Drive\n#from google.colab import drive\n#drive.mount('/content/drive')\n\n#import os\n#os.chdir('/content/drive/MyDrive/kaggle Feedback')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuH7idBliYP8","outputId":"fa563cce-d351-415f-eb19-e01b01264dce","execution":{"iopub.status.busy":"2022-10-10T05:48:45.158784Z","iopub.execute_input":"2022-10-10T05:48:45.159247Z","iopub.status.idle":"2022-10-10T05:48:45.180674Z","shell.execute_reply.started":"2022-10-10T05:48:45.159151Z","shell.execute_reply":"2022-10-10T05:48:45.179741Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"SAVE_PATH = './'\nTRAIN_PATH = '../input/feedback-prize-english-language-learning/train.csv'\nTEST_PATH = '../input/feedback-prize-english-language-learning/test.csv'\nSAMPLE_SUB_PATH = '../input/feedback-prize-english-language-learning/sample_submission.csv' \n\nTARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"id":"sBcRQkP8jGL2","execution":{"iopub.status.busy":"2022-10-10T05:48:45.183009Z","iopub.execute_input":"2022-10-10T05:48:45.183680Z","iopub.status.idle":"2022-10-10T05:48:45.192663Z","shell.execute_reply.started":"2022-10-10T05:48:45.183644Z","shell.execute_reply":"2022-10-10T05:48:45.191655Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install iterative-stratification\n!pip install sentencepiece\n!pip install transformers==4.21.2\n#!pip install iterative-stratification --no-index --find-links=file:../input/iterstratification/iterstrat\n\nimport warnings\nimport sentencepiece\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000) \nfrom tqdm import tqdm\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom torch import autocast\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, BertModel, BertTokenizer\n\nprint('Transformer Version: ', transformers.__version__)","metadata":{"id":"KxsDW5bmDL3v","execution":{"iopub.status.busy":"2022-10-10T05:48:45.194227Z","iopub.execute_input":"2022-10-10T05:48:45.194575Z","iopub.status.idle":"2022-10-10T05:49:24.869911Z","shell.execute_reply.started":"2022-10-10T05:48:45.194541Z","shell.execute_reply":"2022-10-10T05:49:24.868749Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    model= 'microsoft/deberta-v3-base'\n    gradient_checkpointing=True\n    epochs=10\n    eps=1e-6\n    num_workers=4\n    batch_size=10\n    weight_decay=0.01\n    target_cols=TARGET_COLS\n    seed=42\n    n_fold=4\n    train=True\n    #scheduler='cosine' # ['linear', 'cosine']\n    #batch_scheduler=True\n    #num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    print_freq = 100\n    #max_len=512\n\n\n\n\n\ndef import_data(tr =TRAIN_PATH, te =TEST_PATH, sample =SAMPLE_SUB_PATH ):\n  df_train = pd.read_csv(tr)\n  df_test = pd.read_csv(te)\n  submission = pd.read_csv(sample)\n  return df_train, df_test, submission\n\ndef replace_nl(df_train, df_test):\n  df_train['full_text'] = df_train['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl= r'', regex=True)\n  df_test['full_text'] = df_test['full_text'].str.replace(pat=r'[\\n\\r\\t\\\\]', repl=r'', regex=True)\n  return df_train, df_test\n\ndef set_folds(df_train):\n  Fold = MultilabelStratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n  for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[cfg.target_cols])):\n      df_train.loc[val_index, 'fold'] = int(n)\n  df_train['fold'] = df_train['fold'].astype(int)\n  display(df_train.groupby('fold').size())\n  return df_train\n\ndef load_prepare():\n  df_train, df_test, submission = import_data()\n  df_train, df_test = replace_nl(df_train, df_test)\n  df_train=  set_folds(df_train)\n  return df_train, df_test, submission\n\n","metadata":{"id":"JpNOcDU65l2q","execution":{"iopub.status.busy":"2022-10-10T05:49:24.872629Z","iopub.execute_input":"2022-10-10T05:49:24.873762Z","iopub.status.idle":"2022-10-10T05:49:24.889457Z","shell.execute_reply.started":"2022-10-10T05:49:24.873711Z","shell.execute_reply":"2022-10-10T05:49:24.888278Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train, df_test, submission = load_prepare()","metadata":{"id":"oi4-ULuW8-7B","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"556065df-4d40-420c-8cdd-6b804c5282cc","execution":{"iopub.status.busy":"2022-10-10T05:49:24.891264Z","iopub.execute_input":"2022-10-10T05:49:24.891956Z","iopub.status.idle":"2022-10-10T05:49:25.350442Z","shell.execute_reply.started":"2022-10-10T05:49:24.891910Z","shell.execute_reply":"2022-10-10T05:49:25.349523Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"fold\n0    978\n1    977\n2    978\n3    978\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#Dataset Deberta Base\nclass Dataset_Db(torch.utils.data.Dataset):\n\n    def __init__(self, df, target, train = True):\n        self.train = train\n        self.target = target\n        if self.train: self.labels = df[self.target].values\n        self.texts = df[[\"full_text\"]].values\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        batch_texts = self.tokenizer(self.texts[idx][0], \n                                padding='max_length', \n                                max_length = 512, truncation=True#, \n                                #return_tensors=\"pt\"\n                                )\n        \n        for k, v in batch_texts.items():\n          batch_texts[k] = torch.tensor(v, dtype=torch.long)\n\n        if self.train: batch_y = torch.tensor(self.labels[idx], dtype=torch.float)\n        if self.train: return batch_texts, batch_y\n        else: return batch_texts\n\n#Model Deberta Base\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass DBB(nn.Module):\n  def __init__(self, cfg):\n    super().__init__()\n    self.cfg = cfg\n    self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n    self.model = AutoModel.from_pretrained(cfg.model)\n    self.pool = MeanPooling()\n    # self.linear = nn.Linear(self.config.hidden_size, 512)\n    # self.dropout = nn.Dropout(p=0.1)\n    # self.relu = nn.ReLU()\n    self.out = nn.Linear(self.config.hidden_size, 1)\n    self._init_weights(self.out)\n\n  def _init_weights(self, module):\n      if isinstance(module, nn.Linear):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.bias is not None:\n              module.bias.data.zero_()\n      elif isinstance(module, nn.Embedding):\n          module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n          if module.padding_idx is not None:\n              module.weight.data[module.padding_idx].zero_()\n      elif isinstance(module, nn.LayerNorm):\n          module.bias.data.zero_()\n          module.weight.data.fill_(1.0)\n\n  def forward(self, inputs):\n      outputs = self.model(**inputs)\n      last_hidden_states = outputs[0]\n      pooled_output = self.pool(last_hidden_states, inputs['attention_mask'])\n      final_out = self.out(pooled_output)\n      return final_out\n    \n# ====================================================\n# Loss\n# ====================================================\n# class RMSELoss(nn.Module):\n#     def __init__(self, reduction='mean', eps=1e-9):\n#         super().__init__()\n#         self.mse = nn.MSELoss(reduction='none')\n#         self.reduction = reduction\n#         self.eps = eps\n\n#     def forward(self, y_pred, y_true):\n#         loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n#         if self.reduction == 'none':\n#             loss = loss\n#         elif self.reduction == 'sum':\n#             loss = loss.sum()\n#         elif self.reduction == 'mean':\n#             loss = loss.mean()\n#         return loss\n\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n          'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n          'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\n","metadata":{"id":"vqhigdnNAGYq","execution":{"iopub.status.busy":"2022-10-10T05:49:25.352267Z","iopub.execute_input":"2022-10-10T05:49:25.353077Z","iopub.status.idle":"2022-10-10T05:49:25.374030Z","shell.execute_reply.started":"2022-10-10T05:49:25.353037Z","shell.execute_reply":"2022-10-10T05:49:25.372888Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"LU84H01xiSZc"}},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler()\n\nfor target in cfg.target_cols:\n  for val_fold in range(cfg.n_fold):\n    oof_df = pd.DataFrame()\n\n\n    train_folds = df_train[df_train['fold'] != val_fold].reset_index(drop=True)\n    valid_folds = df_train[df_train['fold'] == val_fold].reset_index(drop=True)\n    valid_labels = valid_folds[target].values\n      \n    train_dataset = Dataset_Db(train_folds, target)\n    valid_dataset = Dataset_Db(valid_folds, target)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=cfg.batch_size,\n                              shuffle=True,\n                              num_workers=cfg.num_workers, \n                              pin_memory=True#, \n                              #drop_last=True\n                              )\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=cfg.batch_size * 2,\n                              shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True, drop_last=False)\n    \n    model = DBB(cfg)\n    #torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=cfg.encoder_lr, \n                                                decoder_lr=cfg.decoder_lr,\n                                                weight_decay=cfg.weight_decay)\n    \n    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n\n    criterion = nn.MSELoss() #RMSELoss(reduction=\"mean\")\n\n    best_score = np.inf\n\n    for epoch in range(cfg.epochs):\n      \n      model.train()\n      scaler = torch.cuda.amp.GradScaler(enabled=True)\n\n      losses = []\n      counter = 0\n\n      #train\n      for step, (inputs, labels) in enumerate(train_loader):\n        \n\n\n          for k, v in inputs.items():\n              inputs[k] = v.to(device)\n\n          labels = labels.to(device)\n          batch_size = labels.size(0)\n\n          with torch.cuda.amp.autocast(enabled=True):\n              y_preds = model(inputs)\n              loss = criterion(y_preds, labels)\n\n          losses.append(loss*batch_size)\n          counter += batch_size\n\n          scaler.scale(loss).backward()\n          scaler.step(optimizer)\n          scaler.update()\n          optimizer.zero_grad()\n          total = sum(losses)/counter\n\n          if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n              print(f'Epoch: [{epoch}][{step}/{len(train_loader)}]  \\n',\n                    f'Loss: {total}')\n          \n          \n      #validation\n\n      val_losses = []\n      val_counter = 0\n      preds = []\n      model.eval()\n\n      for step, (inputs, labels) in enumerate(valid_loader):\n\n        for k, v in inputs.items():\n              inputs[k] = v.to(device)\n\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n\n        with torch.no_grad():\n            val_y_preds = model(inputs)\n            val_loss = criterion(val_y_preds, labels)\n\n        val_losses.append(val_loss*batch_size)\n        val_counter += batch_size\n\n        total_val = sum(val_losses)/val_counter\n        preds.append(val_y_preds.to('cpu').numpy())\n\n            \n\n    predictions = np.concatenate(preds)\n    total_val_loss = sum(val_losses)/val_counter\n    print(f'EVAL: Loss: {total_val_loss}')\n\n\n    if best_score > total_val_loss:\n      best_score = total_val_loss\n      torch.save({'model': model.state_dict(),\n                  'predictions': predictions},\n                  SAVE_PATH+f\"{cfg.model.replace('/', '-')}_t{target}_fold{val_fold}_best.pth\")\n      \n    del model\n  \n\n\n\n\n\n\n\n\n\n\n\n\n        ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"3IGJPW0jF1m4","outputId":"fa2a0771-7593-428e-dfcc-8fdedfdc2a47","execution":{"iopub.status.busy":"2022-10-10T05:49:25.377343Z","iopub.execute_input":"2022-10-10T05:49:25.377927Z","iopub.status.idle":"2022-10-10T13:15:00.297196Z","shell.execute_reply.started":"2022-10-10T05:49:25.377896Z","shell.execute_reply":"2022-10-10T13:15:00.295754Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18e188a7f9e48c6a26ff540f021a7a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea942088a1f449ccaab584f629d87a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spm.model:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c04793e795e46b3bc666809f440066e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2753680ac3634cb291b13722999bc92d"}},"metadata":{}},{"name":"stdout","text":"Epoch: [0][0/294]  \n Loss: 6.545257568359375\nEpoch: [0][100/294]  \n Loss: 1.2253577709197998\nEpoch: [0][200/294]  \n Loss: 0.8348015546798706\nEpoch: [0][293/294]  \n Loss: 0.7270554900169373\nEpoch: [1][0/294]  \n Loss: 0.7756169438362122\nEpoch: [1][100/294]  \n Loss: 0.4913107752799988\nEpoch: [1][200/294]  \n Loss: 0.48145005106925964\nEpoch: [1][293/294]  \n Loss: 0.4658658802509308\nEpoch: [2][0/294]  \n Loss: 0.5746942162513733\nEpoch: [2][100/294]  \n Loss: 0.4703650176525116\nEpoch: [2][200/294]  \n Loss: 0.45988667011260986\nEpoch: [2][293/294]  \n Loss: 0.45625728368759155\nEpoch: [3][0/294]  \n Loss: 0.39450761675834656\nEpoch: [3][100/294]  \n Loss: 0.46897661685943604\nEpoch: [3][200/294]  \n Loss: 0.46205195784568787\nEpoch: [3][293/294]  \n Loss: 0.45495179295539856\nEVAL: Loss: 0.4337410628795624\nEpoch: [0][0/294]  \n Loss: 13.98059368133545\nEpoch: [0][100/294]  \n Loss: 1.7808196544647217\nEpoch: [0][200/294]  \n Loss: 1.1188163757324219\nEpoch: [0][293/294]  \n Loss: 0.9168269038200378\nEpoch: [1][0/294]  \n Loss: 0.3416036069393158\nEpoch: [1][100/294]  \n Loss: 0.4644503891468048\nEpoch: [1][200/294]  \n Loss: 0.46585142612457275\nEpoch: [1][293/294]  \n Loss: 0.45421332120895386\nEpoch: [2][0/294]  \n Loss: 0.5892893671989441\nEpoch: [2][100/294]  \n Loss: 0.4467310607433319\nEpoch: [2][200/294]  \n Loss: 0.44746607542037964\nEpoch: [2][293/294]  \n Loss: 0.4488188624382019\nEpoch: [3][0/294]  \n Loss: 0.3100031912326813\nEpoch: [3][100/294]  \n Loss: 0.4567554295063019\nEpoch: [3][200/294]  \n Loss: 0.47057855129241943\nEpoch: [3][293/294]  \n Loss: 0.4528978168964386\nEVAL: Loss: 0.46076858043670654\nEpoch: [0][0/294]  \n Loss: 9.405778884887695\nEpoch: [0][100/294]  \n Loss: 1.2204967737197876\nEpoch: [0][200/294]  \n Loss: 0.8425467610359192\nEpoch: [0][293/294]  \n Loss: 0.7321479916572571\nEpoch: [1][0/294]  \n Loss: 0.25198888778686523\nEpoch: [1][100/294]  \n Loss: 0.4443977475166321\nEpoch: [1][200/294]  \n Loss: 0.43264350295066833\nEpoch: [1][293/294]  \n Loss: 0.44418811798095703\nEpoch: [2][0/294]  \n Loss: 0.12199043482542038\nEpoch: [2][100/294]  \n Loss: 0.4165017306804657\nEpoch: [2][200/294]  \n Loss: 0.43146032094955444\nEpoch: [2][293/294]  \n Loss: 0.44117897748947144\nEpoch: [3][0/294]  \n Loss: 0.6070505380630493\nEpoch: [3][100/294]  \n Loss: 0.43376684188842773\nEpoch: [3][200/294]  \n Loss: 0.4395255744457245\nEpoch: [3][293/294]  \n Loss: 0.4437541961669922\nEVAL: Loss: 0.4537513852119446\nEpoch: [0][0/294]  \n Loss: 4.611754894256592\nEpoch: [0][100/294]  \n Loss: 0.8817474246025085\nEpoch: [0][200/294]  \n Loss: 0.6734866499900818\nEpoch: [0][293/294]  \n Loss: 0.6014689207077026\nEpoch: [1][0/294]  \n Loss: 0.4314303398132324\nEpoch: [1][100/294]  \n Loss: 0.45528900623321533\nEpoch: [1][200/294]  \n Loss: 0.4532530903816223\nEpoch: [1][293/294]  \n Loss: 0.45125654339790344\nEpoch: [2][0/294]  \n Loss: 0.2148303985595703\nEpoch: [2][100/294]  \n Loss: 0.4559774398803711\nEpoch: [2][200/294]  \n Loss: 0.472954124212265\nEpoch: [2][293/294]  \n Loss: 0.45568445324897766\nEpoch: [3][0/294]  \n Loss: 0.2299082726240158\nEpoch: [3][100/294]  \n Loss: 0.4342769384384155\nEpoch: [3][200/294]  \n Loss: 0.4447115957736969\nEpoch: [3][293/294]  \n Loss: 0.4469741880893707\nEVAL: Loss: 0.45174485445022583\nEpoch: [0][0/294]  \n Loss: 3.881505250930786\nEpoch: [0][100/294]  \n Loss: 0.925079882144928\nEpoch: [0][200/294]  \n Loss: 0.6852140426635742\nEpoch: [0][293/294]  \n Loss: 0.5999301075935364\nEpoch: [1][0/294]  \n Loss: 0.38446691632270813\nEpoch: [1][100/294]  \n Loss: 0.43410617113113403\nEpoch: [1][200/294]  \n Loss: 0.4234098494052887\nEpoch: [1][293/294]  \n Loss: 0.4329029619693756\nEpoch: [2][0/294]  \n Loss: 0.8277252316474915\nEpoch: [2][100/294]  \n Loss: 0.4203685224056244\nEpoch: [2][200/294]  \n Loss: 0.425443559885025\nEpoch: [2][293/294]  \n Loss: 0.4238831698894501\nEpoch: [3][0/294]  \n Loss: 0.5023981928825378\nEpoch: [3][100/294]  \n Loss: 0.41473388671875\nEpoch: [3][200/294]  \n Loss: 0.4295880198478699\nEpoch: [3][293/294]  \n Loss: 0.43326401710510254\nEVAL: Loss: 0.4027077257633209\nEpoch: [0][0/294]  \n Loss: 22.185565948486328\nEpoch: [0][100/294]  \n Loss: 2.626340866088867\nEpoch: [0][200/294]  \n Loss: 1.5329936742782593\nEpoch: [0][293/294]  \n Loss: 1.1830308437347412\nEpoch: [1][0/294]  \n Loss: 0.4687817692756653\nEpoch: [1][100/294]  \n Loss: 0.4384562075138092\nEpoch: [1][200/294]  \n Loss: 0.42985495924949646\nEpoch: [1][293/294]  \n Loss: 0.43208882212638855\nEpoch: [2][0/294]  \n Loss: 0.4244507849216461\nEpoch: [2][100/294]  \n Loss: 0.4310128092765808\nEpoch: [2][200/294]  \n Loss: 0.43418389558792114\nEpoch: [2][293/294]  \n Loss: 0.4282730221748352\nEpoch: [3][0/294]  \n Loss: 0.5549938082695007\nEpoch: [3][100/294]  \n Loss: 0.4108917713165283\nEpoch: [3][200/294]  \n Loss: 0.42007482051849365\nEpoch: [3][293/294]  \n Loss: 0.42503321170806885\nEVAL: Loss: 0.41600415110588074\nEpoch: [0][0/294]  \n Loss: 8.740005493164062\nEpoch: [0][100/294]  \n Loss: 1.0881246328353882\nEpoch: [0][200/294]  \n Loss: 0.7513943910598755\nEpoch: [0][293/294]  \n Loss: 0.6477602124214172\nEpoch: [1][0/294]  \n Loss: 0.2735033333301544\nEpoch: [1][100/294]  \n Loss: 0.4235513210296631\nEpoch: [1][200/294]  \n Loss: 0.4218519926071167\nEpoch: [1][293/294]  \n Loss: 0.42268744111061096\nEpoch: [2][0/294]  \n Loss: 0.47623929381370544\nEpoch: [2][100/294]  \n Loss: 0.4226713478565216\nEpoch: [2][200/294]  \n Loss: 0.4271872639656067\nEpoch: [2][293/294]  \n Loss: 0.42397046089172363\nEpoch: [3][0/294]  \n Loss: 0.9625259637832642\nEpoch: [3][100/294]  \n Loss: 0.42174938321113586\nEpoch: [3][200/294]  \n Loss: 0.42853933572769165\nEpoch: [3][293/294]  \n Loss: 0.41897666454315186\nEVAL: Loss: 0.4379720091819763\nEpoch: [0][0/294]  \n Loss: 12.409682273864746\nEpoch: [0][100/294]  \n Loss: 1.8995299339294434\nEpoch: [0][200/294]  \n Loss: 1.1669422388076782\nEpoch: [0][293/294]  \n Loss: 0.9423331022262573\nEpoch: [1][0/294]  \n Loss: 0.8774842619895935\nEpoch: [1][100/294]  \n Loss: 0.443957656621933\nEpoch: [1][200/294]  \n Loss: 0.4250740706920624\nEpoch: [1][293/294]  \n Loss: 0.42808854579925537\nEpoch: [2][0/294]  \n Loss: 0.36143532395362854\nEpoch: [2][200/294]  \n Loss: 0.427928626537323\nEpoch: [2][293/294]  \n Loss: 0.42428088188171387\nEpoch: [3][0/294]  \n Loss: 0.37341102957725525\nEpoch: [3][100/294]  \n Loss: 0.43324387073516846\nEpoch: [3][200/294]  \n Loss: 0.42357245087623596\nEpoch: [3][293/294]  \n Loss: 0.41861432790756226\nEVAL: Loss: 0.45861369371414185\nEpoch: [0][0/294]  \n Loss: 11.26198959350586\nEpoch: [0][100/294]  \n Loss: 1.7315561771392822\nEpoch: [0][200/294]  \n Loss: 1.0481666326522827\nEpoch: [0][293/294]  \n Loss: 0.8358570337295532\nEpoch: [1][0/294]  \n Loss: 0.18491683900356293\nEpoch: [1][100/294]  \n Loss: 0.3469785451889038\nEpoch: [1][200/294]  \n Loss: 0.35069558024406433\nEpoch: [1][293/294]  \n Loss: 0.35668274760246277\nEpoch: [2][0/294]  \n Loss: 0.19074112176895142\nEpoch: [2][100/294]  \n Loss: 0.35486462712287903\nEpoch: [2][200/294]  \n Loss: 0.34934982657432556\nEpoch: [2][293/294]  \n Loss: 0.35612034797668457\nEpoch: [3][0/294]  \n Loss: 0.24382057785987854\nEpoch: [3][100/294]  \n Loss: 0.35677772760391235\nEpoch: [3][200/294]  \n Loss: 0.3582288324832916\nEpoch: [3][293/294]  \n Loss: 0.35680484771728516\nEVAL: Loss: 0.33127039670944214\nEpoch: [0][0/294]  \n Loss: 10.000591278076172\nEpoch: [0][100/294]  \n Loss: 1.2493677139282227\nEpoch: [0][200/294]  \n Loss: 0.8002974987030029\nEpoch: [0][293/294]  \n Loss: 0.6606758236885071\nEpoch: [1][0/294]  \n Loss: 0.3985992968082428\nEpoch: [1][100/294]  \n Loss: 0.3619192838668823\nEpoch: [1][200/294]  \n Loss: 0.35246986150741577\nEpoch: [1][293/294]  \n Loss: 0.350442498922348\nEpoch: [2][0/294]  \n Loss: 0.35977402329444885\nEpoch: [2][100/294]  \n Loss: 0.3575400412082672\nEpoch: [2][200/294]  \n Loss: 0.34929144382476807\nEpoch: [2][293/294]  \n Loss: 0.34737980365753174\nEpoch: [3][0/294]  \n Loss: 0.22839240729808807\nEpoch: [3][100/294]  \n Loss: 0.3421148359775543\nEpoch: [3][200/294]  \n Loss: 0.34415003657341003\nEpoch: [3][293/294]  \n Loss: 0.3528593182563782\nEVAL: Loss: 0.43780145049095154\nEpoch: [0][0/294]  \n Loss: 12.929181098937988\nEpoch: [0][100/294]  \n Loss: 1.451749563217163\nEpoch: [0][200/294]  \n Loss: 0.9054990410804749\nEpoch: [0][293/294]  \n Loss: 0.7234013676643372\nEpoch: [1][0/294]  \n Loss: 0.33684226870536804\nEpoch: [1][100/294]  \n Loss: 0.35311159491539\nEpoch: [1][200/294]  \n Loss: 0.3461313843727112\nEpoch: [1][293/294]  \n Loss: 0.34896594285964966\nEpoch: [2][0/294]  \n Loss: 0.3433947265148163\nEpoch: [2][100/294]  \n Loss: 0.35078462958335876\nEpoch: [2][200/294]  \n Loss: 0.3454172909259796\nEpoch: [2][293/294]  \n Loss: 0.347110390663147\nEpoch: [3][0/294]  \n Loss: 0.19067810475826263\nEpoch: [3][100/294]  \n Loss: 0.3494601845741272\nEpoch: [3][200/294]  \n Loss: 0.3482655882835388\nEpoch: [3][293/294]  \n Loss: 0.3488410711288452\nEVAL: Loss: 0.35535886883735657\nEpoch: [0][0/294]  \n Loss: 11.299160957336426\nEpoch: [0][100/294]  \n Loss: 1.5054309368133545\nEpoch: [0][200/294]  \n Loss: 0.9301849007606506\nEpoch: [0][293/294]  \n Loss: 0.7457008361816406\nEpoch: [1][0/294]  \n Loss: 0.3025668263435364\nEpoch: [1][100/294]  \n Loss: 0.3417195975780487\nEpoch: [1][200/294]  \n Loss: 0.33514565229415894\nEpoch: [1][293/294]  \n Loss: 0.34657642245292664\nEpoch: [2][0/294]  \n Loss: 0.38727155327796936\nEpoch: [2][100/294]  \n Loss: 0.366253525018692\nEpoch: [2][200/294]  \n Loss: 0.35241878032684326\nEpoch: [2][293/294]  \n Loss: 0.3489139676094055\nEpoch: [3][0/294]  \n Loss: 0.26421546936035156\nEpoch: [3][100/294]  \n Loss: 0.36091673374176025\nEpoch: [3][200/294]  \n Loss: 0.34713783860206604\nEpoch: [3][293/294]  \n Loss: 0.34828588366508484\nEVAL: Loss: 0.34400421380996704\nEpoch: [0][0/294]  \n Loss: 11.166658401489258\nEpoch: [0][100/294]  \n Loss: 1.536616563796997\nEpoch: [0][200/294]  \n Loss: 1.012692928314209\nEpoch: [0][293/294]  \n Loss: 0.8318793177604675\nEpoch: [1][0/294]  \n Loss: 0.45354780554771423\nEpoch: [1][100/294]  \n Loss: 0.4410591423511505\nEpoch: [1][200/294]  \n Loss: 0.4425452649593353\nEpoch: [1][293/294]  \n Loss: 0.4521215856075287\nEpoch: [2][0/294]  \n Loss: 0.2821117341518402\nEpoch: [2][100/294]  \n Loss: 0.4517920911312103\nEpoch: [2][200/294]  \n Loss: 0.4374300539493561\nEpoch: [2][293/294]  \n Loss: 0.4432840645313263\nEpoch: [3][0/294]  \n Loss: 0.27678313851356506\nEpoch: [3][100/294]  \n Loss: 0.4274718463420868\nEpoch: [3][200/294]  \n Loss: 0.43974897265434265\nEpoch: [3][293/294]  \n Loss: 0.442310094833374\nEVAL: Loss: 0.44930440187454224\nEpoch: [0][0/294]  \n Loss: 8.503064155578613\nEpoch: [0][100/294]  \n Loss: 1.3482935428619385\nEpoch: [0][200/294]  \n Loss: 0.8925105929374695\nEpoch: [0][293/294]  \n Loss: 0.7515692710876465\nEpoch: [1][0/294]  \n Loss: 0.34596893191337585\nEpoch: [1][100/294]  \n Loss: 0.47909635305404663\nEpoch: [1][200/294]  \n Loss: 0.454992413520813\nEpoch: [1][293/294]  \n Loss: 0.4534035921096802\nEpoch: [2][0/294]  \n Loss: 0.42193588614463806\nEpoch: [2][100/294]  \n Loss: 0.45564597845077515\nEpoch: [2][200/294]  \n Loss: 0.4554492235183716\nEpoch: [2][293/294]  \n Loss: 0.4515882432460785\nEpoch: [3][0/294]  \n Loss: 0.26562172174453735\nEpoch: [3][100/294]  \n Loss: 0.443518728017807\nEpoch: [3][200/294]  \n Loss: 0.44940921664237976\nEpoch: [3][293/294]  \n Loss: 0.4497053921222687\nEVAL: Loss: 0.42339053750038147\nEpoch: [0][0/294]  \n Loss: 8.359269142150879\nEpoch: [0][100/294]  \n Loss: 1.3709555864334106\nEpoch: [0][200/294]  \n Loss: 0.9274877309799194\nEpoch: [0][293/294]  \n Loss: 0.7740323543548584\nEpoch: [1][0/294]  \n Loss: 0.3544139862060547\nEpoch: [1][100/294]  \n Loss: 0.4463661015033722\nEpoch: [1][200/294]  \n Loss: 0.44757091999053955\nEpoch: [1][293/294]  \n Loss: 0.4389364421367645\nEpoch: [2][0/294]  \n Loss: 0.5588790774345398\nEpoch: [2][100/294]  \n Loss: 0.43014201521873474\nEpoch: [2][200/294]  \n Loss: 0.42871326208114624\nEpoch: [2][293/294]  \n Loss: 0.43552693724632263\nEpoch: [3][0/294]  \n Loss: 0.09469451755285263\nEpoch: [3][100/294]  \n Loss: 0.44256630539894104\nEpoch: [3][200/294]  \n Loss: 0.4319365620613098\nEpoch: [3][293/294]  \n Loss: 0.43228983879089355\nEVAL: Loss: 0.5811493992805481\nEpoch: [0][0/294]  \n Loss: 12.772761344909668\nEpoch: [0][100/294]  \n Loss: 1.629963755607605\nEpoch: [0][200/294]  \n Loss: 1.0423275232315063\nEpoch: [0][293/294]  \n Loss: 0.8469930291175842\nEpoch: [1][0/294]  \n Loss: 0.24056625366210938\nEpoch: [1][100/294]  \n Loss: 0.43585801124572754\nEpoch: [1][200/294]  \n Loss: 0.4346649646759033\nEpoch: [1][293/294]  \n Loss: 0.43741872906684875\nEpoch: [2][0/294]  \n Loss: 0.6972306370735168\nEpoch: [2][100/294]  \n Loss: 0.44962796568870544\nEpoch: [2][200/294]  \n Loss: 0.4461289644241333\nEpoch: [2][293/294]  \n Loss: 0.44143515825271606\nEpoch: [3][0/294]  \n Loss: 0.27861669659614563\nEpoch: [3][100/294]  \n Loss: 0.43417930603027344\nEpoch: [3][200/294]  \n Loss: 0.4484298527240753\nEpoch: [3][293/294]  \n Loss: 0.4429423213005066\nEVAL: Loss: 0.48579660058021545\nEpoch: [0][0/294]  \n Loss: 5.799829006195068\nEpoch: [0][100/294]  \n Loss: 1.2479013204574585\nEpoch: [0][200/294]  \n Loss: 0.8830253481864929\nEpoch: [0][293/294]  \n Loss: 0.7637874484062195\nEpoch: [1][0/294]  \n Loss: 0.43790704011917114\nEpoch: [1][100/294]  \n Loss: 0.48271211981773376\nEpoch: [1][200/294]  \n Loss: 0.5062126517295837\nEpoch: [1][293/294]  \n Loss: 0.5048425197601318\nEpoch: [2][0/294]  \n Loss: 0.5413002371788025\nEpoch: [2][100/294]  \n Loss: 0.5298708081245422\nEpoch: [2][200/294]  \n Loss: 0.509080708026886\nEpoch: [2][293/294]  \n Loss: 0.5106655359268188\nEpoch: [3][0/294]  \n Loss: 0.32978859543800354\nEpoch: [3][100/294]  \n Loss: 0.4603763818740845\nEpoch: [3][200/294]  \n Loss: 0.4878913462162018\nEpoch: [3][293/294]  \n Loss: 0.5000742673873901\nEVAL: Loss: 0.5367115139961243\nEpoch: [0][0/294]  \n Loss: 5.426399230957031\nEpoch: [0][100/294]  \n Loss: 0.8730546832084656\nEpoch: [0][200/294]  \n Loss: 0.6943486332893372\nEpoch: [0][293/294]  \n Loss: 0.621146559715271\nEpoch: [1][0/294]  \n Loss: 0.7315666675567627\nEpoch: [1][100/294]  \n Loss: 0.484900563955307\nEpoch: [1][200/294]  \n Loss: 0.4969119429588318\nEpoch: [1][293/294]  \n Loss: 0.4975648820400238\nEpoch: [2][0/294]  \n Loss: 0.2988552749156952\nEpoch: [2][100/294]  \n Loss: 0.48693886399269104\nEpoch: [2][200/294]  \n Loss: 0.5004580616950989\nEpoch: [2][293/294]  \n Loss: 0.4991984963417053\nEpoch: [3][0/294]  \n Loss: 0.7087805867195129\nEpoch: [3][100/294]  \n Loss: 0.5231102108955383\nEpoch: [3][200/294]  \n Loss: 0.5060979723930359\nEpoch: [3][293/294]  \n Loss: 0.5021153688430786\nEVAL: Loss: 0.5092952251434326\nEpoch: [0][0/294]  \n Loss: 14.315402030944824\nEpoch: [0][100/294]  \n Loss: 1.790958046913147\nEpoch: [0][200/294]  \n Loss: 1.1537761688232422\nEpoch: [0][293/294]  \n Loss: 0.9520874619483948\nEpoch: [1][0/294]  \n Loss: 0.7159867882728577\nEpoch: [1][100/294]  \n Loss: 0.49108704924583435\nEpoch: [1][200/294]  \n Loss: 0.4924926459789276\nEpoch: [1][293/294]  \n Loss: 0.5016927123069763\nEpoch: [2][0/294]  \n Loss: 0.21107934415340424\nEpoch: [2][100/294]  \n Loss: 0.5268667936325073\nEpoch: [2][200/294]  \n Loss: 0.5094167590141296\nEpoch: [2][293/294]  \n Loss: 0.507810115814209\nEpoch: [3][0/294]  \n Loss: 0.8658256530761719\nEpoch: [3][100/294]  \n Loss: 0.5050593614578247\nEpoch: [3][200/294]  \n Loss: 0.5068389177322388\nEpoch: [3][293/294]  \n Loss: 0.5058456063270569\nEVAL: Loss: 0.5026126503944397\nEpoch: [0][0/294]  \n Loss: 7.494760036468506\nEpoch: [0][100/294]  \n Loss: 1.257952332496643\nEpoch: [0][200/294]  \n Loss: 0.8796725273132324\nEpoch: [0][293/294]  \n Loss: 0.7717581987380981\nEpoch: [1][0/294]  \n Loss: 0.5128097534179688\nEpoch: [1][100/294]  \n Loss: 0.49241408705711365\nEpoch: [1][200/294]  \n Loss: 0.49527227878570557\nEpoch: [1][293/294]  \n Loss: 0.4969229996204376\nEpoch: [2][0/294]  \n Loss: 0.7766537070274353\nEpoch: [2][100/294]  \n Loss: 0.5155182480812073\nEpoch: [2][200/294]  \n Loss: 0.5051725506782532\nEpoch: [2][293/294]  \n Loss: 0.493988573551178\nEpoch: [3][0/294]  \n Loss: 0.5671353936195374\nEpoch: [3][100/294]  \n Loss: 0.48298346996307373\nEpoch: [3][200/294]  \n Loss: 0.5048567056655884\nEpoch: [3][293/294]  \n Loss: 0.49819567799568176\nEVAL: Loss: 0.5381948947906494\nEpoch: [0][0/294]  \n Loss: 16.72922134399414\nEpoch: [0][100/294]  \n Loss: 1.9459928274154663\nEpoch: [0][200/294]  \n Loss: 1.2177214622497559\nEpoch: [0][293/294]  \n Loss: 0.9851962924003601\nEpoch: [1][0/294]  \n Loss: 0.2732512652873993\nEpoch: [1][100/294]  \n Loss: 0.46083495020866394\nEpoch: [1][200/294]  \n Loss: 0.47373342514038086\nEpoch: [1][293/294]  \n Loss: 0.47408753633499146\nEpoch: [2][0/294]  \n Loss: 0.32619020342826843\nEpoch: [2][100/294]  \n Loss: 0.4578680396080017\nEpoch: [2][200/294]  \n Loss: 0.4580962657928467\nEpoch: [2][293/294]  \n Loss: 0.46548745036125183\nEpoch: [3][0/294]  \n Loss: 0.6956265568733215\nEpoch: [3][100/294]  \n Loss: 0.4710216820240021\nEpoch: [3][200/294]  \n Loss: 0.46101295948028564\nEpoch: [3][293/294]  \n Loss: 0.46102410554885864\nEVAL: Loss: 0.4983798861503601\nEpoch: [0][0/294]  \n Loss: 17.32500648498535\nEpoch: [0][100/294]  \n Loss: 2.557130813598633\nEpoch: [0][200/294]  \n Loss: 1.5193281173706055\nEpoch: [0][293/294]  \n Loss: 1.1924549341201782\nEpoch: [1][0/294]  \n Loss: 0.1749558299779892\nEpoch: [1][100/294]  \n Loss: 0.46219003200531006\nEpoch: [1][200/294]  \n Loss: 0.45897987484931946\nEpoch: [1][293/294]  \n Loss: 0.46338629722595215\nEpoch: [2][0/294]  \n Loss: 0.8790570497512817\nEpoch: [2][100/294]  \n Loss: 0.5055814981460571\nEpoch: [2][200/294]  \n Loss: 0.4691202640533447\nEpoch: [2][293/294]  \n Loss: 0.4632417857646942\nEpoch: [3][0/294]  \n Loss: 0.2921700179576874\nEpoch: [3][100/294]  \n Loss: 0.47140657901763916\nEpoch: [3][200/294]  \n Loss: 0.4533930718898773\nEpoch: [3][293/294]  \n Loss: 0.46115347743034363\nEVAL: Loss: 0.6200464963912964\nEpoch: [0][0/294]  \n Loss: 13.064933776855469\nEpoch: [0][100/294]  \n Loss: 1.595325231552124\nEpoch: [0][200/294]  \n Loss: 1.0281959772109985\nEpoch: [0][293/294]  \n Loss: 0.8532394766807556\nEpoch: [1][0/294]  \n Loss: 0.6955209374427795\nEpoch: [1][100/294]  \n Loss: 0.48658305406570435\nEpoch: [1][200/294]  \n Loss: 0.47744736075401306\nEpoch: [1][293/294]  \n Loss: 0.4710037410259247\nEpoch: [2][0/294]  \n Loss: 0.618743360042572\nEpoch: [2][100/294]  \n Loss: 0.4584248960018158\nEpoch: [2][200/294]  \n Loss: 0.44688335061073303\nEpoch: [2][293/294]  \n Loss: 0.46241623163223267\nEpoch: [3][0/294]  \n Loss: 0.41773149371147156\nEpoch: [3][100/294]  \n Loss: 0.48416683077812195\nEpoch: [3][200/294]  \n Loss: 0.4688816964626312\nEpoch: [3][293/294]  \n Loss: 0.4633006751537323\nEVAL: Loss: 0.4680124819278717\nEpoch: [0][0/294]  \n Loss: 5.080972194671631\nEpoch: [0][100/294]  \n Loss: 0.9722723364830017\nEpoch: [0][200/294]  \n Loss: 0.696901798248291\nEpoch: [0][293/294]  \n Loss: 0.632142961025238\nEpoch: [1][0/294]  \n Loss: 0.310097336769104\nEpoch: [1][100/294]  \n Loss: 0.4777167737483978\nEpoch: [1][200/294]  \n Loss: 0.45725947618484497\nEpoch: [1][293/294]  \n Loss: 0.45439374446868896\nEpoch: [2][0/294]  \n Loss: 0.33043357729911804\nEpoch: [2][100/294]  \n Loss: 0.4574148654937744\nEpoch: [2][200/294]  \n Loss: 0.44614091515541077\nEpoch: [2][293/294]  \n Loss: 0.4539298713207245\nEpoch: [3][0/294]  \n Loss: 0.6432443857192993\nEpoch: [3][100/294]  \n Loss: 0.4162372648715973\nEpoch: [3][200/294]  \n Loss: 0.42631593346595764\nEpoch: [3][293/294]  \n Loss: 0.4523048996925354\nEVAL: Loss: 0.5085609555244446\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('./', 'zip', './')","metadata":{"execution":{"iopub.status.busy":"2022-10-10T13:29:09.801604Z","iopub.execute_input":"2022-10-10T13:29:09.802582Z","iopub.status.idle":"2022-10-10T14:05:06.550142Z","shell.execute_reply.started":"2022-10-10T13:29:09.802543Z","shell.execute_reply":"2022-10-10T14:05:06.549136Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working.zip'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"nVgpoeUSPfCh"},"execution_count":null,"outputs":[]}]}